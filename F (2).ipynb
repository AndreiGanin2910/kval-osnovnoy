{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMi3eoSYU4Yd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th6O5FhI9U2y",
        "outputId": "2eac6f18-c0f6-40bb-c57e-dfb6e72df3d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...\n",
            "Train shape: (70000, 35), Test shape: (25000, 34)\n",
            "–°–æ–∑–¥–∞–µ–º CSV —Ñ–∞–π–ª—ã...\n",
            "–°–æ–∑–¥–∞–µ–º train.csv...\n",
            "CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: train.csv, shape=(70000, 3)\n",
            "–°–æ–∑–¥–∞–µ–º test.csv...\n",
            "CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: test.csv, shape=(25000, 3)\n",
            "üîß –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...\n",
            "üìä –ö–æ–¥–∏—Ä—É–µ–º body_type...\n",
            "üìä –ö–æ–¥–∏—Ä—É–µ–º drive_type...\n",
            "üìä –ö–æ–¥–∏—Ä—É–µ–º engine_type...\n",
            "üìä –ö–æ–¥–∏—Ä—É–µ–º color...\n",
            "üìä –ö–æ–¥–∏—Ä—É–µ–º pts...\n",
            "üìä –ö–æ–¥–∏—Ä—É–µ–º steering_wheel...\n",
            "üéØ –ú—É–ª—å—Ç–∏–≤—ã–±–æ—Ä–Ω—ã–µ –ø–æ–ª—è: 13\n",
            "‚úÖ –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: 32\n",
            "üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...\n",
            "üìä Train samples: 59500, Val samples: 10500\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 168MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞. –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 12,018,114\n",
            "üìà –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 5,694,850\n",
            "üéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 59500 –ø—Ä–∏–º–µ—Ä–∞—Ö...\n",
            "\n",
            "üìç –≠–ø–æ—Ö–∞ 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                               "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ùå –û—à–∏–±–∫–∞: running_mean should contain 4 elements not 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3261624057.py\", line 743, in <cell line: 0>\n",
            "    trained_model, training_history = main_training()\n",
            "                                      ^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3261624057.py\", line 593, in main_training\n",
            "    train_loss = run_epoch(model, train_loader, optimizer, device, loss_fn, scheduler)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3261624057.py\", line 489, in run_epoch\n",
            "    preds_log = model(imgs, tabular)\n",
            "                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3261624057.py\", line 422, in forward\n",
            "    attention_weights = self.attention(visual_feats) / np.sqrt(visual_feats.size(-1))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
            "    return F.batch_norm(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\", line 2817, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: running_mean should contain 4 elements not 256\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "import requests\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==================== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ====================\n",
        "\n",
        "# –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å GPU\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ==================== –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ ====================\n",
        "\n",
        "TRAIN_PQ = \"/content/train_dataset.parquet\"\n",
        "TEST_PQ  = \"/content/test_dataset.parquet\"\n",
        "TRAIN_IMG_DIR = \"train_images\"\n",
        "TEST_IMG_DIR  = \"test_images\"\n",
        "\n",
        "TRAIN_CSV = \"train.csv\"\n",
        "TEST_CSV  = \"test.csv\"\n",
        "CKPT_NAME = \"improved_model\"\n",
        "CKPT_PATH = f\"{CKPT_NAME}.pth\"\n",
        "USE_LOG_TARGET = True\n",
        "\n",
        "# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "IMG_SIZE = 224\n",
        "N_IMAGES_PER_ITEM = 4\n",
        "BATCH_SIZE = 32  # –£–≤–µ–ª–∏—á–∏–ª–∏ –±–∞—Ç—á\n",
        "EPOCHS = 20      # –£–≤–µ–ª–∏—á–∏–ª–∏ —ç–ø–æ—Ö–∏\n",
        "LR = 1e-3        # –£–≤–µ–ª–∏—á–∏–ª–∏ learning rate\n",
        "WEIGHT_DECAY = 1e-6\n",
        "VAL_SIZE = 0.15\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# ==================== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ====================\n",
        "\n",
        "def get_direct_file_link(mailru_file_url: str) -> str:\n",
        "    \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä—è–º–æ–π —Å—Å—ã–ª–∫–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å Mail.ru\"\"\"\n",
        "    resp = requests.get(mailru_file_url)\n",
        "    if resp.status_code != 200:\n",
        "        raise RuntimeError(f\"–û—à–∏–±–∫–∞ {resp.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {mailru_file_url}\")\n",
        "    page = resp.text\n",
        "    match = re.search(r'dispatcher.*?weblink_get.*?url\":\"(.*?)\"', page)\n",
        "    if not match:\n",
        "        raise RuntimeError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ CDN —Å—Å—ã–ª–∫—É –≤ HTML\")\n",
        "    base_url = match.group(1)\n",
        "    parts = mailru_file_url.split('/')[-3:]\n",
        "    return f\"{base_url}/{parts[0]}/{parts[1]}/{parts[2]}\"\n",
        "\n",
        "def download_from_mailru(file_url: str, local_name: str):\n",
        "    \"\"\"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å Mail.ru\"\"\"\n",
        "    direct = get_direct_file_link(file_url)\n",
        "    print(f\"–°–∫–∞—á–∏–≤–∞–µ–º {file_url} ‚Üí {local_name}\")\n",
        "    os.system(f\"wget -q --content-disposition '{direct}' -O '{local_name}'\")\n",
        "\n",
        "# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç\n",
        "if not os.path.exists(TRAIN_IMG_DIR) or len(os.listdir(TRAIN_IMG_DIR)) == 0:\n",
        "    print(\"–°–∫–∞—á–∏–≤–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...\")\n",
        "    train_link = \"https://cloud.mail.ru/public/2kaD/W4xWY9vgr/train_images.zip\"\n",
        "    download_from_mailru(train_link, \"train_images.zip\")\n",
        "    print(\"–†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...\")\n",
        "    os.system('unzip -q train_images.zip -d train_images && rm train_images.zip')\n",
        "\n",
        "if not os.path.exists(TEST_IMG_DIR) or len(os.listdir(TEST_IMG_DIR)) == 0:\n",
        "    print(\"–°–∫–∞—á–∏–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...\")\n",
        "    test_link = \"https://cloud.mail.ru/public/2kaD/W4xWY9vgr/test_images.zip\"\n",
        "    download_from_mailru(test_link, \"test_images.zip\")\n",
        "    print(\"–†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...\")\n",
        "    os.system('unzip -q test_images.zip -d test_images && rm test_images.zip')\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...\")\n",
        "train_df = pd.read_parquet(TRAIN_PQ)\n",
        "test_df = pd.read_parquet(TEST_PQ)\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
        "\n",
        "# ==================== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ====================\n",
        "\n",
        "def build_img_csv(df, img_dir, out_csv, n_images=4):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ CSV —Å –ø—É—Ç—è–º–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º\"\"\"\n",
        "    if os.path.exists(out_csv):\n",
        "        print(f\"–§–∞–π–ª {out_csv} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∑–∞–≥—Ä—É–∂–∞–µ–º...\")\n",
        "        return pd.read_csv(out_csv)\n",
        "\n",
        "    print(f\"–°–æ–∑–¥–∞–µ–º {out_csv}...\")\n",
        "    all_files = os.listdir(img_dir)\n",
        "    id_to_files = defaultdict(list)\n",
        "\n",
        "    for fname in all_files:\n",
        "        if fname.endswith(\".jpg\"):\n",
        "            try:\n",
        "                iid = int(fname.split(\"_\")[0])\n",
        "                id_to_files[iid].append(os.path.join(img_dir, fname))\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "    rows = []\n",
        "    for r in df.itertuples(index=False):\n",
        "        iid = getattr(r, \"ID\")\n",
        "        paths = sorted(id_to_files.get(iid, []))[:n_images]\n",
        "\n",
        "        row = {\n",
        "            \"item_id\": iid,\n",
        "            \"paths\": \";\".join(paths)\n",
        "        }\n",
        "\n",
        "        if \"price_TARGET\" in df.columns:\n",
        "            row[\"price\"] = getattr(r, \"price_TARGET\")\n",
        "        else:\n",
        "            row[\"price\"] = -1\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    csv_df = pd.DataFrame(rows)\n",
        "    csv_df.to_csv(out_csv, index=False)\n",
        "    print(f\"CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {out_csv}, shape={csv_df.shape}\")\n",
        "    return csv_df\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º CSV —Ñ–∞–π–ª—ã —Å –ø—É—Ç—è–º–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º\n",
        "print(\"–°–æ–∑–¥–∞–µ–º CSV —Ñ–∞–π–ª—ã...\")\n",
        "train_csv_df = build_img_csv(train_df, TRAIN_IMG_DIR, TRAIN_CSV, n_images=N_IMAGES_PER_ITEM)\n",
        "test_csv_df = build_img_csv(test_df, TEST_IMG_DIR, TEST_CSV, n_images=N_IMAGES_PER_ITEM)\n",
        "\n",
        "# ==================== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –¢–ê–ë–õ–ò–ß–ù–´–• –î–ê–ù–ù–´–• ====================\n",
        "\n",
        "class TabularPreprocessor:\n",
        "    \"\"\"–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoders = {}\n",
        "        self.feature_cols = []\n",
        "        self.numeric_cols = ['doors_number', 'crashes_count', 'owners_count', 'mileage', 'latitude', 'longitude']\n",
        "        self.categorical_cols = ['body_type', 'drive_type', 'engine_type', 'color', 'pts', 'steering_wheel']\n",
        "\n",
        "    def fit_transform(self, train_df, test_df):\n",
        "        \"\"\"–û–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
        "        print(\"üîß –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...\")\n",
        "\n",
        "        train_processed = train_df.copy()\n",
        "        test_processed = test_df.copy()\n",
        "\n",
        "        # –£–¥–∞–ª—è–µ–º —Å–ª–æ–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
        "        cols_to_drop = ['close_date', 'equipment']\n",
        "        train_processed = train_processed.drop(columns=cols_to_drop, errors='ignore')\n",
        "        test_processed = test_processed.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
        "        for col in self.numeric_cols:\n",
        "            if col in train_processed.columns:\n",
        "                train_processed[col] = pd.to_numeric(train_processed[col], errors='coerce')\n",
        "                test_processed[col] = pd.to_numeric(test_processed[col], errors='coerce')\n",
        "\n",
        "        # –ö–æ–¥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "        for col in self.categorical_cols:\n",
        "            if col in train_processed.columns:\n",
        "                print(f\"üìä –ö–æ–¥–∏—Ä—É–µ–º {col}...\")\n",
        "                combined = pd.concat([train_processed[col], test_processed[col]]).fillna('Unknown')\n",
        "                le = LabelEncoder()\n",
        "                le.fit(combined.astype(str))\n",
        "                self.label_encoders[col] = le\n",
        "\n",
        "                train_processed[col] = le.transform(train_processed[col].fillna('Unknown').astype(str))\n",
        "                test_processed[col] = le.transform(test_processed[col].fillna('Unknown').astype(str))\n",
        "\n",
        "        # –ú—É–ª—å—Ç–∏–≤—ã–±–æ—Ä–Ω—ã–µ –ø–æ–ª—è\n",
        "        multiselect_cols = [col for col in train_processed.columns if '_mult' in col]\n",
        "        print(f\"üéØ –ú—É–ª—å—Ç–∏–≤—ã–±–æ—Ä–Ω—ã–µ –ø–æ–ª—è: {len(multiselect_cols)}\")\n",
        "\n",
        "        for col in multiselect_cols:\n",
        "            train_processed[f'{col}_count'] = train_processed[col].apply(\n",
        "                lambda x: len(x) if isinstance(x, list) and x != [None] else 0\n",
        "            )\n",
        "            test_processed[f'{col}_count'] = test_processed[col].apply(\n",
        "                lambda x: len(x) if isinstance(x, list) and x != [None] else 0\n",
        "            )\n",
        "\n",
        "        # –û–¥–Ω–æ–≤—ã–±–æ—Ä–Ω—ã–µ –æ–ø—Ü–∏–∏\n",
        "        single_select_cols = ['audiosistema', 'diski', 'electropodemniki', 'fary', 'salon', 'upravlenie_klimatom', 'usilitel_rul']\n",
        "        single_select_cols = [col for col in single_select_cols if col in train_processed.columns]\n",
        "\n",
        "        for col in single_select_cols:\n",
        "            train_processed[col] = train_processed[col].notna().astype(int)\n",
        "            test_processed[col] = test_processed[col].notna().astype(int)\n",
        "\n",
        "        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "        self.feature_cols = (self.categorical_cols + self.numeric_cols +\n",
        "                           [f'{col}_count' for col in multiselect_cols] +\n",
        "                           single_select_cols)\n",
        "        self.feature_cols = [col for col in self.feature_cols if col in train_processed.columns]\n",
        "\n",
        "        print(f\"‚úÖ –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.feature_cols)}\")\n",
        "\n",
        "        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "        for col in self.feature_cols:\n",
        "            if col in self.numeric_cols or '_count' in col:\n",
        "                median_val = train_processed[col].median()\n",
        "                train_processed[col] = train_processed[col].fillna(median_val)\n",
        "                test_processed[col] = test_processed[col].fillna(median_val)\n",
        "            else:\n",
        "                mode_val = train_processed[col].mode()[0] if len(train_processed[col].mode()) > 0 else 0\n",
        "                train_processed[col] = train_processed[col].fillna(mode_val)\n",
        "                test_processed[col] = test_processed[col].fillna(mode_val)\n",
        "\n",
        "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "        numeric_features = [col for col in self.feature_cols if col in self.numeric_cols or '_count' in col]\n",
        "        if numeric_features:\n",
        "            train_processed[numeric_features] = self.scaler.fit_transform(train_processed[numeric_features])\n",
        "            test_processed[numeric_features] = self.scaler.transform(test_processed[numeric_features])\n",
        "\n",
        "        return train_processed, test_processed\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "preprocessor = TabularPreprocessor()\n",
        "train_tabular, test_tabular = preprocessor.fit_transform(train_df, test_df)\n",
        "feature_cols = preprocessor.feature_cols\n",
        "\n",
        "# ==================== –£–õ–£–ß–®–ï–ù–ù–´–ô DATASET –ò –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ò ====================\n",
        "\n",
        "# –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
        "train_tfms = A.Compose([\n",
        "    A.LongestMaxSize(IMG_SIZE),\n",
        "    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.1),\n",
        "    A.RandomRotate90(p=0.3),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
        "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
        "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "valid_tfms = A.Compose([\n",
        "    A.LongestMaxSize(IMG_SIZE),\n",
        "    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "def _safe_imread(p: str, fallback_hw=(IMG_SIZE, IMG_SIZE)) -> np.ndarray:\n",
        "    \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º\"\"\"\n",
        "    img = cv2.imread(p, cv2.IMREAD_COLOR)\n",
        "    if img is None:\n",
        "        h, w = fallback_hw\n",
        "        img = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    else:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "class ImprovedCarsDataset(Dataset):\n",
        "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏\"\"\"\n",
        "\n",
        "    def __init__(self, csv_df: pd.DataFrame, tabular_df: pd.DataFrame, feature_cols: list, is_train: bool = True):\n",
        "        self.csv_df = csv_df.reset_index(drop=True)\n",
        "        self.tabular_df = tabular_df.reset_index(drop=True)\n",
        "        self.feature_cols = feature_cols\n",
        "        self.tfms = train_tfms if is_train else valid_tfms\n",
        "        self.has_target = \"price\" in csv_df.columns\n",
        "        self.is_train = is_train\n",
        "\n",
        "        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º —Ç–∞–±–ª–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "        self.tabular_features = {}\n",
        "        for idx, row in self.csv_df.iterrows():\n",
        "            item_id = row[\"item_id\"]\n",
        "            tabular_mask = self.tabular_df['ID'] == item_id\n",
        "            if tabular_mask.any():\n",
        "                tabular_row = self.tabular_df[tabular_mask].iloc[0]\n",
        "                self.tabular_features[idx] = tabular_row[self.feature_cols].values.astype(np.float32)\n",
        "            else:\n",
        "                self.tabular_features[idx] = np.zeros(len(feature_cols), dtype=np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv_df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.csv_df.iloc[idx]\n",
        "        item_id = row[\"item_id\"]\n",
        "\n",
        "        # –¢–∞–±–ª–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω—ã)\n",
        "        tabular_features = torch.tensor(self.tabular_features[idx], dtype=torch.float32)\n",
        "\n",
        "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
        "        paths = [p for p in str(row[\"paths\"]).split(\";\") if p and p.lower() != \"nan\"][:N_IMAGES_PER_ITEM]\n",
        "        imgs = []\n",
        "\n",
        "        for p in paths:\n",
        "            img = _safe_imread(p)\n",
        "            img = self.tfms(image=img)[\"image\"]\n",
        "            imgs.append(img)\n",
        "\n",
        "        # –î–æ–±–∏–≤–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
        "        while len(imgs) < N_IMAGES_PER_ITEM:\n",
        "            imgs.append(torch.zeros(3, IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "        if self.has_target:\n",
        "            y = torch.tensor(row[\"price\"], dtype=torch.float32)\n",
        "            if USE_LOG_TARGET:\n",
        "                y = torch.log1p(y)\n",
        "            return imgs, tabular_features, y, torch.tensor(row[\"price\"], dtype=torch.float32)  # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ü–µ–Ω—É\n",
        "        else:\n",
        "            return imgs, tabular_features, torch.tensor(item_id, dtype=torch.long)\n",
        "\n",
        "# ==================== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ú–û–î–ï–õ–¨ ====================\n",
        "\n",
        "class ImprovedMultiImageResNet(nn.Module):\n",
        "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π\"\"\"\n",
        "\n",
        "    def __init__(self, tabular_input_size=0, use_tabular=True, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.use_tabular = use_tabular\n",
        "\n",
        "        # –í–∏–∑—É–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å —Å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–º–∏ –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ —Å–ª–æ—è–º–∏\n",
        "        backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "        in_features_visual = backbone.fc.in_features\n",
        "\n",
        "        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏\n",
        "        for param in list(backbone.parameters())[:50]:\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.visual_backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
        "        self.visual_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # –£–ª—É—á—à–µ–Ω–Ω—ã–π attention –º–µ—Ö–∞–Ω–∏–∑–º\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(in_features_visual, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ç–∞–±–ª–∏—á–Ω–∞—è —á–∞—Å—Ç—å\n",
        "        if use_tabular:\n",
        "            self.tabular_net = nn.Sequential(\n",
        "                nn.Linear(tabular_input_size, 512),\n",
        "                nn.BatchNorm1d(512),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.BatchNorm1d(256),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dropout_rate/2),\n",
        "                nn.Linear(256, 128),\n",
        "            )\n",
        "            combined_features = in_features_visual + 128\n",
        "        else:\n",
        "            combined_features = in_features_visual\n",
        "\n",
        "        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(combined_features, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate/2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –Ω–æ–≤—ã—Ö —Å–ª–æ–µ–≤\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, images, tabular_features=None):\n",
        "        B, N, C, H, W = images.shape\n",
        "\n",
        "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
        "        images_flat = images.view(B * N, C, H, W)\n",
        "        visual_feats = self.visual_backbone(images_flat)\n",
        "        visual_feats = self.visual_pool(visual_feats).view(B * N, -1)\n",
        "        visual_feats = visual_feats.view(B, N, -1)\n",
        "\n",
        "        # Attention —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π –¥–ª—è —Å–º—è–≥—á–µ–Ω–∏—è softmax\n",
        "        attention_weights = self.attention(visual_feats) / np.sqrt(visual_feats.size(-1))\n",
        "        attention_weights = F.softmax(attention_weights, dim=1)\n",
        "        weighted_visual = (visual_feats * attention_weights).sum(dim=1)\n",
        "\n",
        "        # –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "        if self.use_tabular and tabular_features is not None:\n",
        "            tabular_feats = self.tabular_net(tabular_features)\n",
        "            combined = torch.cat([weighted_visual, tabular_feats], dim=1)\n",
        "        else:\n",
        "            combined = weighted_visual\n",
        "\n",
        "        output = self.head(combined)\n",
        "        return output.squeeze(1)\n",
        "\n",
        "# ==================== –£–õ–£–ß–®–ï–ù–ù–´–ï –§–£–ù–ö–¶–ò–ò –ü–û–¢–ï–†–¨ ====================\n",
        "\n",
        "class ImprovedCombinedLoss(nn.Module):\n",
        "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å\"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.8, smooth_l1_beta=1.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.smooth_l1 = nn.SmoothL1Loss(beta=smooth_l1_beta)\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        smooth_l1_loss = self.smooth_l1(y_pred, y_true)\n",
        "        mse_loss = self.mse(y_pred, y_true)\n",
        "        return self.alpha * smooth_l1_loss + (1 - self.alpha) * mse_loss\n",
        "\n",
        "class RMSLELoss(nn.Module):\n",
        "    \"\"\"RMSLE Loss –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Ü–µ–Ω\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        return torch.sqrt(self.mse(torch.log1p(y_pred), torch.log1p(y_true)))\n",
        "\n",
        "# ==================== –£–õ–£–ß–®–ï–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï ====================\n",
        "\n",
        "def run_epoch(model, loader, optimizer=None, device=\"cuda\", loss_fn=None, scheduler=None):\n",
        "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —ç–ø–æ—Ö–∏ —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º\"\"\"\n",
        "    is_train = optimizer is not None\n",
        "    model.train(is_train)\n",
        "\n",
        "    losses = []\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ\n",
        "    accumulation_steps = 4\n",
        "    pbar = tqdm(loader, desc=\"Train\" if is_train else \"Valid\", leave=False)\n",
        "\n",
        "    for i, batch in enumerate(pbar):\n",
        "        if len(batch) == 4:  # train/val\n",
        "            imgs, tabular, y_log, y_original = batch\n",
        "        else:  # test\n",
        "            imgs, tabular, ids = batch\n",
        "            y_log = y_original = None\n",
        "\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        tabular = tabular.to(device, non_blocking=True)\n",
        "\n",
        "        if is_train and y_log is not None:\n",
        "            y_log = y_log.to(device, non_blocking=True)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                preds_log = model(imgs, tabular)\n",
        "                loss = loss_fn(preds_log, y_log) / accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                if scheduler:\n",
        "                    scheduler.step()\n",
        "\n",
        "            losses.append(loss.item() * accumulation_steps)\n",
        "\n",
        "        elif not is_train and y_log is not None:\n",
        "            y_log = y_log.to(device, non_blocking=True)\n",
        "            y_original = y_original.to(device, non_blocking=True)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                preds_log = model(imgs, tabular)\n",
        "                loss = loss_fn(preds_log, y_log)\n",
        "                losses.append(loss.item())\n",
        "\n",
        "                # –î–ª—è –º–µ—Ç—Ä–∏–∫ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ\n",
        "                preds_original = torch.expm1(preds_log)\n",
        "                all_preds.append(preds_original.cpu())\n",
        "                all_targets.append(y_original.cpu())\n",
        "\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.3f}\"})\n",
        "\n",
        "    if is_train:\n",
        "        return float(np.mean(losses)) if losses else 0.0\n",
        "    else:\n",
        "        if all_preds:\n",
        "            all_preds = torch.cat(all_preds).numpy()\n",
        "            all_targets = torch.cat(all_targets).numpy()\n",
        "            medape = median_absolute_percentage_error(all_targets, all_preds) * 100.0\n",
        "            return float(np.mean(losses)), float(medape)\n",
        "        return 0.0, 0.0\n",
        "\n",
        "def median_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ–¥–∏–∞–Ω–Ω–æ–π –∞–±—Å–æ–ª—é—Ç–Ω–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–π –æ—à–∏–±–∫–∏\"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    epsilon = 1e-6\n",
        "    ape = 2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred) + epsilon)\n",
        "    return float(np.median(ape))\n",
        "\n",
        "# ==================== –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø ====================\n",
        "\n",
        "def main_training():\n",
        "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
        "    print(\"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...\")\n",
        "\n",
        "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "    df = pd.read_csv(TRAIN_CSV)\n",
        "    trn_df, val_df = train_test_split(df, test_size=VAL_SIZE, random_state=SEED, shuffle=True)\n",
        "\n",
        "    print(f\"üìä Train samples: {len(trn_df)}, Val samples: {len(val_df)}\")\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
        "    train_ds = ImprovedCarsDataset(trn_df, train_tabular, feature_cols, is_train=True)\n",
        "    valid_ds = ImprovedCarsDataset(val_df, train_tabular, feature_cols, is_train=False)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                             pin_memory=True, num_workers=4, drop_last=True,\n",
        "                             persistent_workers=True)\n",
        "    valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             pin_memory=True, num_workers=4, drop_last=False,\n",
        "                             persistent_workers=True)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "    model = ImprovedMultiImageResNet(\n",
        "        tabular_input_size=len(feature_cols),\n",
        "        use_tabular=True,\n",
        "        dropout_rate=0.3\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"üß† –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞. –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"üìà –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
        "    loss_fn = ImprovedCombinedLoss(alpha=0.7)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, betas=(0.9, 0.999))\n",
        "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
        "\n",
        "    # Automatic Mixed Precision\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    history = {\n",
        "        \"epoch\": [], \"train_loss\": [], \"val_loss\": [],\n",
        "        \"val_medape\": [], \"lr\": []\n",
        "    }\n",
        "\n",
        "    best_val_medape = float(\"inf\")\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"üéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(train_ds)} –ø—Ä–∏–º–µ—Ä–∞—Ö...\")\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        print(f\"\\nüìç –≠–ø–æ—Ö–∞ {epoch}/{EPOCHS}\")\n",
        "\n",
        "        # –û–±—É—á–µ–Ω–∏–µ\n",
        "        train_loss = run_epoch(model, train_loader, optimizer, device, loss_fn, scheduler)\n",
        "\n",
        "        # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
        "        val_loss, val_medape = run_epoch(model, valid_loader, None, device, loss_fn)\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f\"üìâ Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MedAPE: {val_medape:.2f}%\")\n",
        "        print(f\"üìö Learning Rate: {current_lr:.2e}\")\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é\n",
        "        history[\"epoch\"].append(epoch)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_medape\"].append(val_medape)\n",
        "        history[\"lr\"].append(current_lr)\n",
        "\n",
        "        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞\n",
        "        if val_medape < best_val_medape:\n",
        "            best_val_medape = val_medape\n",
        "            patience_counter = 0\n",
        "\n",
        "            torch.save({\n",
        "                \"model\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"scheduler\": scheduler.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "                \"val_medape\": val_medape,\n",
        "                \"history\": history,\n",
        "                \"feature_cols\": feature_cols\n",
        "            }, CKPT_PATH)\n",
        "            print(f\"üíæ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —ç–ø–æ—Ö—É {epoch} -> {CKPT_PATH}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch}\")\n",
        "                break\n",
        "\n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "    plot_training_history(history)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(history[\"epoch\"], history[\"val_medape\"], 'r-', label=\"Validation MedAPE\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Median APE, %\")\n",
        "    plt.title(\"Validation Median APE\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(history[\"epoch\"], history[\"train_loss\"], 'b-', label=\"Train Loss\")\n",
        "    plt.plot(history[\"epoch\"], history[\"val_loss\"], 'r-', label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(history[\"epoch\"], history[\"lr\"], 'g-')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Learning Rate\")\n",
        "    plt.title(\"Learning Rate Schedule\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"improved_training_history.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é\n",
        "    pd.DataFrame(history).to_csv(f\"{CKPT_NAME}_training_history.csv\", index=False)\n",
        "\n",
        "# ==================== –£–õ–£–ß–®–ï–ù–ù–´–ô –ò–ù–§–ï–†–ï–ù–° ====================\n",
        "\n",
        "def create_final_submission(model_path=None):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–∞–±–º–∏—Ç–∞\"\"\"\n",
        "    print(\"üéØ –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–∞–±–º–∏—Ç...\")\n",
        "\n",
        "    if model_path is None:\n",
        "        model_path = CKPT_PATH\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "    model = ImprovedMultiImageResNet(\n",
        "        tabular_input_size=len(feature_cols),\n",
        "        use_tabular=True\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model.eval()\n",
        "\n",
        "    # –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "    test_ds = ImprovedCarsDataset(test_csv_df, test_tabular, feature_cols, is_train=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE * 2, shuffle=False,\n",
        "                            num_workers=4, pin_memory=True, drop_last=False)\n",
        "\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    predictions_dict = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Test Inference\"):\n",
        "            if len(batch) == 3:  # test data\n",
        "                imgs, tabular, ids = batch\n",
        "                imgs = imgs.to(device, non_blocking=True)\n",
        "                tabular = tabular.to(device, non_blocking=True)\n",
        "\n",
        "                preds_log = model(imgs, tabular)\n",
        "                preds = torch.expm1(preds_log) if USE_LOG_TARGET else preds_log\n",
        "\n",
        "                batch_ids = ids.cpu().numpy()\n",
        "                batch_preds = preds.cpu().numpy()\n",
        "\n",
        "                for item_id, pred in zip(batch_ids, batch_preds):\n",
        "                    predictions_dict[item_id] = pred\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Å–∞–±–º–∏—Ç\n",
        "    submission_rows = []\n",
        "    for item_id in test_df['ID'].values:\n",
        "        pred = predictions_dict.get(item_id, np.mean(list(predictions_dict.values())))\n",
        "        submission_rows.append({'ID': item_id, 'price_TARGET': pred})\n",
        "\n",
        "    submission_df = pd.DataFrame(submission_rows)\n",
        "    submission_file = f\"final_submission_{CKPT_NAME}.csv\"\n",
        "    submission_df.to_csv(submission_file, index=False)\n",
        "\n",
        "    print(f\"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∞–±–º–∏—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {submission_file}\")\n",
        "    print(f\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:\")\n",
        "    print(f\"   - –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞: {submission_df['price_TARGET'].min():.2f} —Ä—É–±.\")\n",
        "    print(f\"   - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞: {submission_df['price_TARGET'].max():.2f} —Ä—É–±.\")\n",
        "    print(f\"   - –ú–µ–¥–∏–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞: {submission_df['price_TARGET'].median():.2f} —Ä—É–±.\")\n",
        "    print(f\"   - –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞: {submission_df['price_TARGET'].mean():.2f} —Ä—É–±.\")\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# ==================== –ó–ê–ü–£–°–ö ====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "        trained_model, training_history = main_training()\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º —Å–∞–±–º–∏—Ç\n",
        "        submission = create_final_submission()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üéâ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
        "        print(f\"üèÜ –õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è MedAPE: {min(training_history['val_medape']):.2f}%\")\n",
        "        print(f\"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {CKPT_PATH}\")\n",
        "        print(f\"üìÑ –°–∞–±–º–∏—Ç –≥–æ—Ç–æ–≤: final_submission_{CKPT_NAME}.csv\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå –û—à–∏–±–∫–∞: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.decomposition import PCA\n",
        "import xgboost as xgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò\n",
        "# =============================================================================\n",
        "\n",
        "def median_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç median absolute percentage error (medianAPE)\"\"\"\n",
        "    mask = y_true > 0\n",
        "    if mask.sum() == 0:\n",
        "        return 1.0  # –ï—Å–ª–∏ –≤—Å–µ –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è <= 0, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ö—É–¥—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "    ape = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
        "    return float(np.median(ape))\n",
        "\n",
        "# =============================================================================\n",
        "# –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "train_df = pd.read_parquet('/content/train_dataset.parquet')\n",
        "test_df = pd.read_parquet('/content/test_dataset.parquet')\n",
        "print(f\"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {train_df.shape}\")\n",
        "print(f\"–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {test_df.shape}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø–∞–ø–æ–∫ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
        "TRAIN_IMG_DIR = '/content/train_images'\n",
        "TEST_IMG_DIR = '/content/test_images'\n",
        "\n",
        "print(f\"–ü–∞–ø–∫–∞ train_images —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(TRAIN_IMG_DIR)}\")\n",
        "print(f\"–ü–∞–ø–∫–∞ test_images —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(TEST_IMG_DIR)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# –î–ê–¢–ê–°–ï–¢ –ò –ú–û–î–ï–õ–¨ –î–õ–Ø –î–ï–¢–ï–ö–¶–ò–ò –ü–û–í–†–ï–ñ–î–ï–ù–ò–ô\n",
        "# =============================================================================\n",
        "\n",
        "class SafeCarDamageDataset(Dataset):\n",
        "    def __init__(self, df, image_dir, transform=None, target_prices=None, img_size=224):\n",
        "        self.df = df\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.target_prices = target_prices\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        car_id = self.df.iloc[idx]['ID']\n",
        "\n",
        "        # –ò—â–µ–º –ø–µ—Ä–≤–æ–µ –¥–æ—Å—Ç—É–ø–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
        "        image_path = None\n",
        "        for i in range(4):  # –ø—Ä–æ–±—É–µ–º 0-3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "            potential_path = f\"{self.image_dir}/{car_id}_{i}.jpg\"\n",
        "            if os.path.exists(potential_path):\n",
        "                image_path = potential_path\n",
        "                break\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
        "        if image_path:\n",
        "            try:\n",
        "                image = cv2.imread(image_path)\n",
        "                if image is None:\n",
        "                    raise ValueError(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}\")\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            except Exception as e:\n",
        "                print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {image_path}: {e}\")\n",
        "                image = np.ones((self.img_size, self.img_size, 3), dtype=np.uint8) * 128\n",
        "        else:\n",
        "            # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω\n",
        "            image = np.ones((self.img_size, self.img_size, 3), dtype=np.uint8) * 128\n",
        "\n",
        "        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "        h, w = image.shape[:2]\n",
        "        if h < self.img_size or w < self.img_size:\n",
        "            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "            scale = max(self.img_size / h, self.img_size / w)\n",
        "            new_h, new_w = int(h * scale), int(w * scale)\n",
        "            image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
        "        if self.transform:\n",
        "            try:\n",
        "                image = self.transform(image=image)['image']\n",
        "            except Exception as e:\n",
        "                print(f\"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è car_id {car_id}: {e}\")\n",
        "                # –°–æ–∑–¥–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
        "                image = torch.ones(3, self.img_size, self.img_size) * 0.5\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º target –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π\n",
        "        if self.target_prices is not None:\n",
        "            price = self.target_prices.iloc[idx]\n",
        "            # –°—á–∏—Ç–∞–µ–º –∞–≤—Ç–æ–º–æ–±–∏–ª—å \"—É–±–∏—Ç—ã–º\" –µ—Å–ª–∏ —Ü–µ–Ω–∞ –≤ –Ω–∏–∂–Ω–∏—Ö 15%\n",
        "            is_damaged = 1.0 if price < np.percentile(self.target_prices, 15) else 0.0\n",
        "            return image, torch.tensor(is_damaged, dtype=torch.float32)\n",
        "\n",
        "        return image, torch.tensor(0.0, dtype=torch.float32)  # –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∞\n",
        "\n",
        "class DamageDetector(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()  # —É–±–∏—Ä–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\n",
        "\n",
        "        # –î–µ—Ç–µ–∫—Ç–æ—Ä –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π\n",
        "        self.damage_classifier = nn.Sequential(\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_classes),\n",
        "            nn.Sigmoid() if num_classes == 1 else nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # –†–µ–≥—Ä–µ—Å—Å–æ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è features\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 64)  # –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ —Ñ–∏—á–∏ –¥–ª—è XGBoost\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        features = self.backbone(x)\n",
        "        damage_score = self.damage_classifier(features)\n",
        "\n",
        "        if return_features:\n",
        "            compact_features = self.feature_extractor(features)\n",
        "            return damage_score, compact_features\n",
        "\n",
        "        return damage_score\n",
        "\n",
        "def get_safe_transforms(mode='train', img_size=224):\n",
        "    \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ª—é–±–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞\"\"\"\n",
        "    if mode == 'train':\n",
        "        return A.Compose([\n",
        "            A.LongestMaxSize(img_size * 2),  # –°–Ω–∞—á–∞–ª–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º\n",
        "            A.PadIfNeeded(\n",
        "                min_height=img_size,\n",
        "                min_width=img_size,\n",
        "                border_mode=cv2.BORDER_CONSTANT,\n",
        "                value=0\n",
        "            ),\n",
        "            A.RandomCrop(img_size, img_size, p=0.8),\n",
        "            A.Resize(img_size, img_size, p=1.0),  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.3),\n",
        "            A.GaussNoise(p=0.2),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    else:  # validation/test\n",
        "        return A.Compose([\n",
        "            A.LongestMaxSize(img_size * 2),\n",
        "            A.PadIfNeeded(\n",
        "                min_height=img_size,\n",
        "                min_width=img_size,\n",
        "                border_mode=cv2.BORDER_CONSTANT,\n",
        "                value=0\n",
        "            ),\n",
        "            A.CenterCrop(img_size, img_size),\n",
        "            A.Resize(img_size, img_size),  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "def train_damage_detector(train_df, image_dir, device='cuda', img_size=224):\n",
        "    \"\"\"–û–±—É—á–∞–µ—Ç –¥–µ—Ç–µ–∫—Ç–æ—Ä –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π\"\"\"\n",
        "    print(\"–û–±—É—á–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π...\")\n",
        "\n",
        "    # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
        "    train_transform = get_safe_transforms('train', img_size)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
        "    dataset = SafeCarDamageDataset(\n",
        "        train_df,\n",
        "        image_dir,\n",
        "        transform=train_transform,\n",
        "        target_prices=train_df['price_TARGET'],\n",
        "        img_size=img_size\n",
        "    )\n",
        "\n",
        "    # DataLoader —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=16,  # –£–º–µ–Ω—å—à–∏–ª batch_size –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "        shuffle=True,\n",
        "        num_workers=2,   # –£–º–µ–Ω—å—à–∏–ª –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    # –ú–æ–¥–µ–ª—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
        "    model = DamageDetector().to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
        "\n",
        "    # –û–±—É—á–µ–Ω–∏–µ\n",
        "    model.train()\n",
        "    for epoch in range(3):  # –£–º–µ–Ω—å—à–∏–ª —ç–ø–æ—Ö–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
        "        total_loss = 0\n",
        "        for batch_idx, (images, targets) in enumerate(dataloader):\n",
        "            try:\n",
        "                images, targets = images.to(device), targets.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs.squeeze(), targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                if batch_idx % 50 == 0:\n",
        "                    print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f'–û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {batch_idx}: {e}')\n",
        "                continue\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f'Epoch {epoch} completed. Average Loss: {total_loss/len(dataloader):.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def extract_damage_features(model, df, image_dir, device='cuda', img_size=224):\n",
        "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∏—á–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\"\"\"\n",
        "    print(\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π...\")\n",
        "\n",
        "    # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
        "    transform = get_safe_transforms('val', img_size)\n",
        "\n",
        "    dataset = SafeCarDamageDataset(df, image_dir, transform=transform, img_size=img_size)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    damage_scores = []\n",
        "    damage_features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _ in dataloader:\n",
        "            try:\n",
        "                images = images.to(device)\n",
        "                scores, features = model(images, return_features=True)\n",
        "\n",
        "                damage_scores.extend(scores.cpu().numpy())\n",
        "                damage_features.extend(features.cpu().numpy())\n",
        "            except Exception as e:\n",
        "                print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞: {e}\")\n",
        "                # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤—ã–µ —Ñ–∏—á–∏ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ –±–∞—Ç—á–∞\n",
        "                batch_size = images.size(0) if hasattr(images, 'size') else 32\n",
        "                damage_scores.extend([0.0] * batch_size)\n",
        "                damage_features.extend([np.zeros(64)] * batch_size)\n",
        "\n",
        "    return np.array(damage_scores).flatten(), np.array(damage_features)\n",
        "\n",
        "# =============================================================================\n",
        "# –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê (–±–µ–∑ category dtype)\n",
        "# =============================================================================\n",
        "\n",
        "def enhanced_preprocess_with_damage(df, damage_scores, damage_features, is_train=True):\n",
        "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å —É—á–µ—Ç–æ–º –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π\"\"\"\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # 1. –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π score –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π\n",
        "    df_processed['damage_score'] = damage_scores\n",
        "\n",
        "    # 2. –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "    if is_train and len(damage_scores) > 0:\n",
        "        damage_threshold = np.percentile(damage_scores, 20)  # 20% —Å–∞–º—ã—Ö \"—É–±–∏—Ç—ã—Ö\"\n",
        "        well_maintained_threshold = np.percentile(damage_scores, 80)  # 20% —Å–∞–º—ã—Ö —É—Ö–æ–∂–µ–Ω–Ω—ã—Ö\n",
        "    else:\n",
        "        damage_threshold = 0.7  # –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥\n",
        "        well_maintained_threshold = 0.3\n",
        "\n",
        "    df_processed['is_severely_damaged'] = (df_processed['damage_score'] > damage_threshold).astype(int)\n",
        "    df_processed['is_well_maintained'] = (df_processed['damage_score'] < well_maintained_threshold).astype(int)\n",
        "\n",
        "    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –≤–º–µ—Å—Ç–æ category dtype –∏—Å–ø–æ–ª—å–∑—É–µ–º —á–∏—Å–ª–æ–≤–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "    damage_bins = [0, 0.3, 0.7, 1.0]\n",
        "    damage_labels = [0, 1, 2]  # good=0, average=1, damaged=2\n",
        "    df_processed['damage_category'] = pd.cut(\n",
        "        df_processed['damage_score'],\n",
        "        bins=damage_bins,\n",
        "        labels=damage_labels\n",
        "    ).astype(int)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ int\n",
        "\n",
        "    # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π\n",
        "    numeric_columns = ['mileage', 'crashes_count', 'owners_count', 'latitude', 'longitude']\n",
        "\n",
        "    for col in numeric_columns:\n",
        "        if col in df_processed.columns:\n",
        "            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
        "            # –ó–∞–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∏–∞–Ω–∞–º–∏ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ damage_category\n",
        "            if is_train:\n",
        "                for category in df_processed['damage_category'].unique():\n",
        "                    mask = df_processed['damage_category'] == category\n",
        "                    median_val = df_processed.loc[mask, col].median()\n",
        "                    df_processed.loc[mask, col] = df_processed.loc[mask, col].fillna(median_val)\n",
        "            else:\n",
        "                df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
        "\n",
        "    # 4. Feature engineering —Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è–º–∏\n",
        "    if 'mileage' in df_processed.columns:\n",
        "        df_processed['log_mileage'] = np.log1p(df_processed['mileage'])\n",
        "        df_processed['mileage_damage_interaction'] = df_processed['mileage'] * df_processed['damage_score']\n",
        "        df_processed['high_mileage_damaged'] = (\n",
        "            (df_processed['mileage'] > df_processed['mileage'].median()) &\n",
        "            (df_processed['is_severely_damaged'] == 1)\n",
        "        ).astype(int)\n",
        "\n",
        "    if 'crashes_count' in df_processed.columns:\n",
        "        df_processed['has_crashes'] = (df_processed['crashes_count'] > 0).astype(int)\n",
        "        df_processed['crashes_damage_interaction'] = df_processed['crashes_count'] * df_processed['damage_score']\n",
        "        df_processed['crashed_and_damaged'] = (\n",
        "            (df_processed['has_crashes'] == 1) &\n",
        "            (df_processed['is_severely_damaged'] == 1)\n",
        "        ).astype(int)\n",
        "\n",
        "    if 'owners_count' in df_processed.columns:\n",
        "        df_processed['single_owner'] = (df_processed['owners_count'] == 1).astype(int)\n",
        "        df_processed['many_owners'] = (df_processed['owners_count'] > 3).astype(int)\n",
        "        df_processed['multiple_owners_damaged'] = (\n",
        "            (df_processed['many_owners'] == 1) &\n",
        "            (df_processed['is_severely_damaged'] == 1)\n",
        "        ).astype(int)\n",
        "\n",
        "    # 5. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫–∏\n",
        "    categorical_columns = ['body_type', 'drive_type', 'engine_type', 'color', 'pts', 'steering_wheel']\n",
        "\n",
        "    for col in categorical_columns:\n",
        "        if col in df_processed.columns:\n",
        "            df_processed[col] = df_processed[col].fillna('unknown').astype(str)\n",
        "\n",
        "    # 6. –û–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–µ –æ–ø—Ü–∏–∏ - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫–∏\n",
        "    single_select_columns = [\n",
        "        'audiosistema', 'diski', 'electropodemniki', 'fary', 'salon',\n",
        "        'upravlenie_klimatom', 'usilitel_rul'\n",
        "    ]\n",
        "\n",
        "    for col in single_select_columns:\n",
        "        if col in df_processed.columns:\n",
        "            df_processed[col] = df_processed[col].fillna('unknown').astype(str)\n",
        "\n",
        "    # 7. –ú—É–ª—å—Ç–∏–≤—ã–±–æ—Ä–Ω—ã–µ –ø–æ–ª—è\n",
        "    multiselect_columns = [col for col in df_processed.columns if '_mult' in col]\n",
        "\n",
        "    for col in multiselect_columns:\n",
        "        if col in df_processed.columns:\n",
        "            try:\n",
        "                df_processed[f'{col}_count'] = df_processed[col].apply(\n",
        "                    lambda x: len(x) if isinstance(x, list) else (1 if pd.notna(x) and x != 'None' else 0)\n",
        "                )\n",
        "                # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏\n",
        "                df_processed[f'{col}_damaged'] = (\n",
        "                    (df_processed[f'{col}_count'] > 0) &\n",
        "                    (df_processed['is_severely_damaged'] == 1)\n",
        "                ).astype(int)\n",
        "            except:\n",
        "                df_processed[f'{col}_count'] = 0\n",
        "                df_processed[f'{col}_damaged'] = 0\n",
        "\n",
        "    for col in multiselect_columns:\n",
        "        if col in df_processed.columns:\n",
        "            df_processed = df_processed.drop(col, axis=1)\n",
        "\n",
        "    # 8. –î–æ–±–∞–≤–ª—è–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π\n",
        "    for i in range(damage_features.shape[1]):\n",
        "        df_processed[f'damage_feature_{i}'] = damage_features[:, i]\n",
        "\n",
        "    # 9. –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–º–µ—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–∏–ø\n",
        "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "# =============================================================================\n",
        "# –£–õ–£–ß–®–ï–ù–ù–´–ô XGBOOST –° –ü–†–ê–í–ò–õ–¨–ù–û–ô –ü–û–î–ì–û–¢–û–í–ö–û–ô –î–ê–ù–ù–´–•\n",
        "# =============================================================================\n",
        "\n",
        "def prepare_data_for_xgboost(X, X_test):\n",
        "    \"\"\"–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è XGBoost, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –≤—Å–µ –≤ —á–∏—Å–ª–æ–≤—ã–µ —Ç–∏–ø—ã\"\"\"\n",
        "    print(\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è XGBoost...\")\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏–∏\n",
        "    X_encoded = X.copy()\n",
        "    X_test_encoded = X_test.copy()\n",
        "\n",
        "    # 1. –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
        "    categorical_columns = [col for col in X_encoded.columns if X_encoded[col].dtype == 'object']\n",
        "    print(f\"–ù–∞–π–¥–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {len(categorical_columns)}\")\n",
        "\n",
        "    # 2. Label Encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    label_encoders = {}\n",
        "    for col in categorical_columns:\n",
        "        le = LabelEncoder()\n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º train –∏ test –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "        combined = pd.concat([X_encoded[col], X_test_encoded[col]], axis=0)\n",
        "        le.fit(combined.astype(str))\n",
        "        X_encoded[col] = le.transform(X_encoded[col].astype(str))\n",
        "        X_test_encoded[col] = le.transform(X_test_encoded[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö\n",
        "    print(\"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è:\")\n",
        "    print(X_encoded.dtypes.value_counts())\n",
        "\n",
        "    # 4. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    numeric_columns = X_encoded.select_dtypes(include=[np.number]).columns\n",
        "    print(f\"–ß–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: {len(numeric_columns)}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = X_encoded.copy()\n",
        "    X_test_scaled = X_test_encoded.copy()\n",
        "\n",
        "    X_scaled[numeric_columns] = scaler.fit_transform(X_encoded[numeric_columns])\n",
        "    X_test_scaled[numeric_columns] = scaler.transform(X_test_encoded[numeric_columns])\n",
        "\n",
        "    # 5. –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤\n",
        "    for col in X_scaled.columns:\n",
        "        if X_scaled[col].dtype.name == 'category':\n",
        "            X_scaled[col] = X_scaled[col].astype(int)\n",
        "        if X_test_scaled[col].dtype.name == 'category':\n",
        "            X_test_scaled[col] = X_test_scaled[col].astype(int)\n",
        "\n",
        "    print(\"–§–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    print(X_scaled.dtypes.value_counts())\n",
        "\n",
        "    return X_scaled, X_test_scaled, label_encoders, scaler\n",
        "\n",
        "def train_xgboost_with_validation(X, y, X_test):\n",
        "    \"\"\"–û–±—É—á–∞–µ—Ç XGBoost —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π\"\"\"\n",
        "    print(\"–û–±—É—á–µ–Ω–∏–µ XGBoost...\")\n",
        "\n",
        "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.15, random_state=42, shuffle=True\n",
        "    )\n",
        "\n",
        "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost\n",
        "    params = {\n",
        "        'n_estimators': 2000,\n",
        "        'max_depth': 8,\n",
        "        'learning_rate': 0.005,\n",
        "        'subsample': 0.9,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'reg_alpha': 0.1,\n",
        "        'reg_lambda': 1.0,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'eval_metric': 'mae',\n",
        "        'early_stopping_rounds': 350\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "\n",
        "    print(\"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è XGBoost...\")\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=100\n",
        "    )\n",
        "\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "    y_pred = model.predict(X_val)\n",
        "    score = median_absolute_percentage_error(y_val, y_pred)\n",
        "    print(f\"XGBoost medianAPE: {score:.4f}\")\n",
        "\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–µ\n",
        "    test_predictions = model.predict(X_test)\n",
        "\n",
        "    return model, test_predictions, score\n",
        "\n",
        "# =============================================================================\n",
        "# –ü–û–õ–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù –° –û–ë–£–ß–ï–ù–ò–ï–ú CNN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
        "\n",
        "    try:\n",
        "        # 1. –û–±—É—á–∞–µ–º –¥–µ—Ç–µ–∫—Ç–æ—Ä –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π\n",
        "        print(\"=\" * 50)\n",
        "        print(\"–û–ë–£–ß–ï–ù–ò–ï CNN –î–ï–¢–ï–ö–¢–û–†–ê –ü–û–í–†–ï–ñ–î–ï–ù–ò–ô\")\n",
        "        print(\"=\" * 50)\n",
        "        damage_model = train_damage_detector(train_df, TRAIN_IMG_DIR, device, img_size=224)\n",
        "\n",
        "        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"–ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –ü–û–í–†–ï–ñ–î–ï–ù–ò–ô\")\n",
        "        print(\"=\" * 50)\n",
        "        train_damage_scores, train_damage_features = extract_damage_features(\n",
        "            damage_model, train_df, TRAIN_IMG_DIR, device, img_size=224\n",
        "        )\n",
        "        test_damage_scores, test_damage_features = extract_damage_features(\n",
        "            damage_model, test_df, TEST_IMG_DIR, device, img_size=224\n",
        "        )\n",
        "\n",
        "        print(f\"–ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π train: {train_damage_features.shape}\")\n",
        "        print(f\"–ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π test: {test_damage_features.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: {e}\")\n",
        "        print(\"–°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∫–∞–∫ fallback...\")\n",
        "        # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Ñ–∏—á–∏ –∫–∞–∫ fallback\n",
        "        train_damage_scores = np.random.uniform(0, 1, len(train_df))\n",
        "        test_damage_scores = np.random.uniform(0, 1, len(test_df))\n",
        "        train_damage_features = np.random.normal(0, 1, (len(train_df), 64))\n",
        "        test_damage_features = np.random.normal(0, 1, (len(test_df), 64))\n",
        "\n",
        "    # 3. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å —É—á–µ—Ç–æ–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"–ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•\")\n",
        "    print(\"=\" * 50)\n",
        "    train_processed = enhanced_preprocess_with_damage(\n",
        "        train_df, train_damage_scores, train_damage_features, is_train=True\n",
        "    )\n",
        "    test_processed = enhanced_preprocess_with_damage(\n",
        "        test_df, test_damage_scores, test_damage_features, is_train=False\n",
        "    )\n",
        "\n",
        "    print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ train –¥–∞–Ω–Ω—ã–µ: {train_processed.shape}\")\n",
        "    print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ test –¥–∞–Ω–Ω—ã–µ: {test_processed.shape}\")\n",
        "\n",
        "    # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è XGBoost\n",
        "    common_columns = list(set(train_processed.columns) & set(test_processed.columns))\n",
        "    if 'price_TARGET' in common_columns:\n",
        "        common_columns.remove('price_TARGET')\n",
        "\n",
        "    X = train_processed[common_columns]\n",
        "    y = train_processed['price_TARGET']\n",
        "    X_test = test_processed[common_columns]\n",
        "\n",
        "    print(f\"\\n–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {X.shape}\")\n",
        "    print(f\"–î–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞: {X_test.shape}\")\n",
        "\n",
        "    # 5. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è XGBoost\n",
        "    X_prepared, X_test_prepared, label_encoders, scaler = prepare_data_for_xgboost(X, X_test)\n",
        "\n",
        "    # 6. –û–±—É—á–µ–Ω–∏–µ XGBoost\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"–û–ë–£–ß–ï–ù–ò–ï XGBOOST\")\n",
        "    print(\"=\" * 50)\n",
        "    model, test_predictions, score = train_xgboost_with_validation(X_prepared, y, X_test_prepared)\n",
        "\n",
        "    # 7. –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"–ü–û–°–¢–û–ë–†–ê–ë–û–¢–ö–ê –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Ü–µ–Ω –¥–ª—è –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π\n",
        "    damage_adjustment = 0.15  # —Å–Ω–∏–∂–∞–µ–º –Ω–∞ 15% –¥–ª—è —Å–∏–ª—å–Ω–æ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö\n",
        "    severely_damaged_mask = test_processed['is_severely_damaged'] == 1\n",
        "    test_predictions[severely_damaged_mask] = test_predictions[severely_damaged_mask] * (1 - damage_adjustment)\n",
        "\n",
        "    print(f\"–°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–æ —Ü–µ–Ω –¥–ª—è {severely_damaged_mask.sum()} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π\")\n",
        "\n",
        "    # –û–±—Ä–µ–∑–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "    q1 = np.percentile(test_predictions, 1)\n",
        "    q99 = np.percentile(test_predictions, 99)\n",
        "    test_predictions = np.clip(test_predictions, q1, q99)\n",
        "\n",
        "    print(f\"–î–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {test_predictions.min():.0f} - {test_predictions.max():.0f}\")\n",
        "\n",
        "    # 8. –°–æ–∑–¥–∞–Ω–∏–µ —Å–∞–±–º–∏—à–µ–Ω–∞\n",
        "    submission = pd.DataFrame({\n",
        "        'ID': test_df['ID'],\n",
        "        'target': test_predictions\n",
        "    })\n",
        "\n",
        "    submission.to_csv('xgboost_with_cnn_damage_detector.csv', index=False)\n",
        "    print(\"\\n–°–∞–±–º–∏—à–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ xgboost_with_cnn_damage_detector.csv\")\n",
        "\n",
        "    # 9. –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"–ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í\")\n",
        "    print(\"=\" * 50)\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_prepared.columns,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(\"\\n–¢–æ–ø-15 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "    print(feature_importance.head(15))\n",
        "\n",
        "    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π\n",
        "    damage_related_features = [col for col in feature_importance['feature']\n",
        "                              if 'damage' in col.lower() or 'feature_' in col]\n",
        "    print(f\"\\n–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –≤ —Ç–æ–ø–µ: {len(damage_related_features)}\")\n",
        "    if damage_related_features:\n",
        "        print(\"–í–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π:\")\n",
        "        for feat in damage_related_features[:10]:\n",
        "            imp = feature_importance[feature_importance['feature'] == feat]['importance'].values[0]\n",
        "            print(f\"  {feat}: {imp:.4f}\")\n",
        "\n",
        "    return submission, model, feature_importance\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    submission, model, feature_importance = main()\n",
        "    print(\"\\nüéØ –ü–∞–π–ø–ª–∞–π–Ω —Å –æ–±—É—á–µ–Ω–∏–µ–º CNN —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!\")"
      ],
      "metadata": {
        "id": "8j7Znk8EhEgF",
        "outputId": "e3885c20-6cf2-4d00-ec3f-c1e7c5e04aab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•\n",
            "==================================================\n",
            "–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: (70000, 35)\n",
            "–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: (25000, 34)\n",
            "–ü–∞–ø–∫–∞ train_images —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: True\n",
            "–ü–∞–ø–∫–∞ test_images —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: True\n",
            "–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n",
            "==================================================\n",
            "–û–ë–£–ß–ï–ù–ò–ï CNN –î–ï–¢–ï–ö–¢–û–†–ê –ü–û–í–†–ï–ñ–î–ï–ù–ò–ô\n",
            "==================================================\n",
            "–û–±—É—á–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π...\n"
          ]
        }
      ]
    }
  ]
}