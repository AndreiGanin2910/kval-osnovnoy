{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egjL8C-wTXtK",
        "outputId": "ba9364f0-8a4d-451c-99c8-e70a4d7c6f1f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/train.tsv.zip\n",
        "! unzip /content/test.tsv.zip\n",
        "! unzip /content/reviews.txv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yygdW_RDTbpK",
        "outputId": "f888776d-892a-4361-fc53-f372513e7c33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/train.tsv.zip\n",
            "  inflating: train.tsv               \n",
            "Archive:  /content/test.tsv.zip\n",
            "  inflating: test.tsv                \n",
            "Archive:  /content/reviews.txv.zip\n",
            "  inflating: reviews.tsv             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import gc\n",
        "import ast\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import GroupKFold, KFold\n",
        "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.neighbors import BallTree\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "# =============== SEED ===============\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# =============== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===============\n",
        "print(\"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "train = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
        "test = pd.read_csv(\"test.tsv\", sep=\"\\t\")\n",
        "reviews = pd.read_csv(\"reviews.tsv\", sep=\"\\t\")\n",
        "\n",
        "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä train: {len(train)}\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä test: {len(test)}\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä reviews: {len(reviews)}\")\n",
        "\n",
        "# –£–¥–∞–ª—è–µ–º target=0 (–Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ)\n",
        "print(\"üßπ –£–¥–∞–ª—è–µ–º –æ–±—ä–µ–∫—Ç—ã —Å target=0 –∏–∑ train...\")\n",
        "removed = int((train[\"target\"] == 0).sum())\n",
        "train = train[train[\"target\"] != 0].reset_index(drop=True)\n",
        "print(f\"–£–¥–∞–ª–µ–Ω–æ: {removed} | –ù–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä train: {len(train)}\")\n",
        "\n",
        "# =============== –ê–ì–†–ï–ì–ê–¶–ò–Ø –û–¢–ó–´–í–û–í ===============\n",
        "print(\"üìä –ê–≥–≥—Ä–µ–≥–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ –ø–æ id...\")\n",
        "reviews_agg = reviews.groupby('id', as_index=False).agg(\n",
        "    all_reviews_text=('text', lambda x: ' '.join(str(t) for t in x if pd.notna(t)))\n",
        ")\n",
        "review_counts = reviews.groupby('id', as_index=False).size().rename(columns={'size': 'review_count'})\n",
        "review_counts['review_count'] = np.log1p(review_counts['review_count'])\n",
        "\n",
        "for df in (train, test):\n",
        "    df.drop(columns=[c for c in [\"all_reviews_text\", \"review_count\"] if c in df.columns], inplace=True, errors='ignore')\n",
        "\n",
        "train = train.merge(reviews_agg, on='id', how='left')\n",
        "train = train.merge(review_counts, on='id', how='left')\n",
        "test = test.merge(reviews_agg, on='id', how='left')\n",
        "test = test.merge(review_counts, on='id', how='left')\n",
        "\n",
        "for df in [train, test]:\n",
        "    df['all_reviews_text'] = df['all_reviews_text'].fillna('')\n",
        "    df['review_count'] = df['review_count'].fillna(0.0).astype(np.float32)\n",
        "\n",
        "# =============== –ö–û–û–†–î–ò–ù–ê–¢–´ –ò –ë–ê–ó–û–í–ê–Ø –ì–ï–û–ì–†–ê–§–ò–Ø ===============\n",
        "print(\"üìç –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –±–∞–∑–æ–≤–∞—è –≥–µ–æ–≥—Ä–∞—Ñ–∏—è...\")\n",
        "def parse_coords_to_lon_lat(s):\n",
        "    try:\n",
        "        if isinstance(s, str):\n",
        "            coords = ast.literal_eval(s)\n",
        "        else:\n",
        "            coords = s\n",
        "        lon, lat = coords[0], coords[1]\n",
        "        return lon, lat\n",
        "    except Exception:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "def haversine_km(lon1, lat1, lon2, lat2):\n",
        "    R = 6371.0\n",
        "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    return R * c\n",
        "\n",
        "for df in [train, test]:\n",
        "    lon_lat = df[\"coordinates\"].apply(parse_coords_to_lon_lat)\n",
        "    df[\"lon\"] = lon_lat.apply(lambda x: x[0]).astype(float)\n",
        "    df[\"lat\"] = lon_lat.apply(lambda x: x[1]).astype(float)\n",
        "\n",
        "for df in [train, test]:\n",
        "    df[\"lon\"] = df[\"lon\"].fillna(df[\"lon\"].median())\n",
        "    df[\"lat\"] = df[\"lat\"].fillna(df[\"lat\"].median())\n",
        "\n",
        "# —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ü–µ–Ω—Ç—Ä–∞ –ú–æ—Å–∫–≤—ã\n",
        "moscow_center_lat, moscow_center_lon = 55.7558, 37.6173\n",
        "for df in [train, test]:\n",
        "    df[\"dist_to_center_km\"] = haversine_km(\n",
        "        df[\"lon\"].values, df[\"lat\"].values,\n",
        "        np.full(len(df), moscow_center_lon), np.full(len(df), moscow_center_lat)\n",
        "    ).astype(np.float32)\n",
        "    df[\"log_dist_to_center\"] = np.log1p(df[\"dist_to_center_km\"]).astype(np.float32)\n",
        "\n",
        "# –ì–µ–æ-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è\n",
        "all_coords = pd.concat([train[[\"lat\", \"lon\"]], test[[\"lat\", \"lon\"]]], axis=0).reset_index(drop=True)\n",
        "scaler = StandardScaler()\n",
        "coords_scaled = scaler.fit_transform(all_coords.values)\n",
        "kmeans = MiniBatchKMeans(n_clusters=50, random_state=SEED, batch_size=2048, n_init=10, max_no_improvement=100)\n",
        "clusters = kmeans.fit_predict(coords_scaled)\n",
        "train[\"geo_cluster\"] = clusters[:len(train)]\n",
        "test[\"geo_cluster\"] = clusters[len(train):]\n",
        "\n",
        "# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≥–µ–æ-—Ñ–∏—á–∏\n",
        "print(\"üß≠ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≥–µ–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏...\")\n",
        "for df in [train, test]:\n",
        "    df[\"lat_rad\"] = np.deg2rad(df[\"lat\"].astype(float))\n",
        "    df[\"lon_rad\"] = np.deg2rad(df[\"lon\"].astype(float))\n",
        "    df[\"sin_lat\"] = np.sin(df[\"lat_rad\"]).astype(np.float32)\n",
        "    df[\"cos_lat\"] = np.cos(df[\"lat_rad\"]).astype(np.float32)\n",
        "    df[\"sin_lon\"] = np.sin(df[\"lon_rad\"]).astype(np.float32)\n",
        "    df[\"cos_lon\"] = np.cos(df[\"lon_rad\"]).astype(np.float32)\n",
        "    # –ø—Ä–æ—Å—Ç–∞—è –º–µ—Ä–∫–∞—Ç–æ—Ä-–ø—Ä–æ–µ–∫—Ü–∏—è\n",
        "    df[\"merc_x\"] = df[\"lon_rad\"].astype(np.float32)\n",
        "    merc_y = np.log(np.tan(np.pi/4 + df[\"lat_rad\"]/2))\n",
        "    merc_y = np.where(np.isfinite(merc_y), merc_y, 0.0)\n",
        "    df[\"merc_y\"] = merc_y.astype(np.float32)\n",
        "\n",
        "# —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ü–µ–Ω—Ç—Ä–∞ –≥–µ–æ-–∫–ª–∞—Å—Ç–µ—Ä–∞\n",
        "geo_centers = train.groupby(\"geo_cluster\")[[\"lat\",\"lon\"]].mean()\n",
        "def dist_to_cluster_center_row(row):\n",
        "    gc_id = row[\"geo_cluster\"]\n",
        "    if gc_id in geo_centers.index:\n",
        "        cl = geo_centers.loc[gc_id]\n",
        "        return haversine_km(row[\"lon\"], row[\"lat\"], cl[\"lon\"], cl[\"lat\"])\n",
        "    return np.nan\n",
        "\n",
        "for df in [train, test]:\n",
        "    df[\"dist_to_cluster_center\"] = df.apply(dist_to_cluster_center_row, axis=1).astype(np.float32)\n",
        "    df[\"dist_to_cluster_center\"] = df[\"dist_to_cluster_center\"].fillna(df[\"dist_to_cluster_center\"].median())\n",
        "\n",
        "# =============== *_300m/_1000m –§–ò–ß–ò (ratio/diff/log) ===============\n",
        "def build_scope_features(train, test):\n",
        "    all_cols = set(train.columns).intersection(set(test.columns))\n",
        "    base_pairs = []\n",
        "    for col in all_cols:\n",
        "        if col.endswith(\"_300m\"):\n",
        "            base = col[:-5]\n",
        "            mate = base + \"_1000m\"\n",
        "            if mate in all_cols:\n",
        "                base_pairs.append(base)\n",
        "    MAX_BASES = 200\n",
        "    if len(base_pairs) > MAX_BASES:\n",
        "        base_pairs = sorted(base_pairs)[:MAX_BASES]\n",
        "\n",
        "    def add_feats(df):\n",
        "        for base in base_pairs:\n",
        "            c300 = base + \"_300m\"; c1000 = base + \"_1000m\"\n",
        "            if c300 in df.columns: df[f\"{base}_300m_log1p\"] = np.log1p(df[c300].astype(float)).astype(np.float32)\n",
        "            if c1000 in df.columns: df[f\"{base}_1000m_log1p\"] = np.log1p(df[c1000].astype(float)).astype(np.float32)\n",
        "            df[f\"{base}_diff_1000_300\"] = (df.get(c1000, 0.0).astype(float) - df.get(c300, 0.0).astype(float)).astype(np.float32)\n",
        "            df[f\"{base}_ratio_300_1000\"] = ((df.get(c300, 0.0).astype(float) + 1.0) / (df.get(c1000, 0.0).astype(float) + 1.0)).astype(np.float32)\n",
        "        return df\n",
        "\n",
        "    train_out = add_feats(train.copy())\n",
        "    test_out = add_feats(test.copy())\n",
        "    return train_out, test_out, base_pairs\n",
        "\n",
        "train, test, scope_bases = build_scope_features(train, test)\n",
        "print(f\"üîé –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ {len(scope_bases)} –ø–∞—Ä *_300m/_1000m.\")\n",
        "\n",
        "# =============== OOF TARGET ENCODING (—É—Ç–∏–ª–∏—Ç–∞) ===============\n",
        "def oof_target_encoding(train_df, test_df, col, target_col, n_splits=5, groups=None, smoothing=10.0, global_mean=None):\n",
        "    if global_mean is None:\n",
        "        global_mean = train_df[target_col].mean()\n",
        "    te_train = np.zeros(len(train_df), dtype=np.float32)\n",
        "    splitter = GroupKFold(n_splits=n_splits) if groups is not None else KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "    for trn_idx, val_idx in splitter.split(train_df, train_df[target_col], groups=groups):\n",
        "        trn_data = train_df.iloc[trn_idx]\n",
        "        stats = trn_data.groupby(col, as_index=True)[target_col].agg(['mean', 'count'])\n",
        "        smoothed = (stats['mean'] * stats['count'] + global_mean * smoothing) / (stats['count'] + smoothing)\n",
        "        te_map = smoothed.to_dict()\n",
        "        te_train[val_idx] = train_df.iloc[val_idx][col].map(te_map).fillna(global_mean).values.astype(np.float32)\n",
        "    stats_full = train_df.groupby(col, as_index=True)[target_col].agg(['mean', 'count'])\n",
        "    smoothed_full = (stats_full['mean'] * stats_full['count'] + global_mean * smoothing) / (stats['count'] + smoothing)\n",
        "    smoothed_full = (stats_full['mean'] * stats_full['count'] + global_mean * smoothing) / (stats_full['count'] + smoothing)\n",
        "    te_map_full = smoothed_full.to_dict()\n",
        "    te_test = test_df[col].map(te_map_full).fillna(global_mean).values.astype(np.float32)\n",
        "    return te_train, te_test\n",
        "\n",
        "global_mean = train[\"target\"].mean()\n",
        "\n",
        "# =============== –ì–ï–û-–¢–ê–ô–õ–´ + OOF-TE ===============\n",
        "print(\"üß± –ì–µ–æ-—Ç–∞–π–ª—ã 500–º/1000–º –∏ OOF-TE...\")\n",
        "def make_geo_tiles(df):\n",
        "    df[\"tile_500m\"] = (np.floor(df[\"lat\"].astype(float)/0.005).astype(int).astype(str) + \"_\" +\n",
        "                       np.floor(df[\"lon\"].astype(float)/0.005).astype(int).astype(str))\n",
        "    df[\"tile_1000m\"] = (np.floor(df[\"lat\"].astype(float)/0.010).astype(int).astype(str) + \"_\" +\n",
        "                        np.floor(df[\"lon\"].astype(float)/0.010).astype(int).astype(str))\n",
        "    return [\"tile_500m\", \"tile_1000m\"]\n",
        "\n",
        "tile_cols = make_geo_tiles(train)\n",
        "_ = make_geo_tiles(test)\n",
        "\n",
        "for col in tile_cols:\n",
        "    te_tr, te_te = oof_target_encoding(\n",
        "        train, test, col=col, target_col=\"target\",\n",
        "        n_splits=5, groups=train[\"geo_cluster\"], smoothing=10.0, global_mean=global_mean\n",
        "    )\n",
        "    train[f\"{col}_te\"] = te_tr\n",
        "    test[f\"{col}_te\"] = te_te\n",
        "\n",
        "    for df in [train, test]:\n",
        "        df[f\"{col}_x_category\"] = df[col].astype(str) + \"|\" + df[\"category\"].astype(str)\n",
        "    te_tr2, te_te2 = oof_target_encoding(\n",
        "        train, test, col=f\"{col}_x_category\", target_col=\"target\",\n",
        "        n_splits=5, groups=train[\"geo_cluster\"], smoothing=20.0, global_mean=global_mean\n",
        "    )\n",
        "    train[f\"{col}_x_category_te\"] = te_tr2\n",
        "    test[f\"{col}_x_category_te\"] = te_te2\n",
        "\n",
        "# =============== TF-IDF –î–õ–Ø –ù–ê–ó–í–ê–ù–ò–ô (char) + SVD ===============\n",
        "print(\"üî§ TF-IDF –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º (char 3‚Äì5) + SVD...\")\n",
        "all_names = pd.concat([train['name'].fillna(''), test['name'].fillna('')], axis=0).astype(str).tolist()\n",
        "vec_name = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), min_df=2, max_features=30000, lowercase=True, sublinear_tf=True)\n",
        "X_name = vec_name.fit_transform(all_names)\n",
        "svd_name = TruncatedSVD(n_components=16, random_state=SEED)\n",
        "X_name_svd = svd_name.fit_transform(X_name)\n",
        "name_svd_cols = [f\"name_tfidf_svd_{i}\" for i in range(X_name_svd.shape[1])]\n",
        "name_svd_train = pd.DataFrame(X_name_svd[:len(train)], columns=name_svd_cols, index=train.index).astype(np.float32)\n",
        "name_svd_test  = pd.DataFrame(X_name_svd[len(train):], columns=name_svd_cols, index=test.index).astype(np.float32)\n",
        "del X_name, X_name_svd; gc.collect()\n",
        "print(f\"‚úÖ EVR (name): {svd_name.explained_variance_ratio_.sum():.3f}\")\n",
        "\n",
        "# =============== –£–ü–†–û–©–ï–ù–ù–ê–Ø –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –û–¢–ó–´–í–û–í + TF-IDF (word/char) + SVD + –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ===============\n",
        "print(\"üßº –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∑—ã–≤–æ–≤ –∏ TF-IDF –ø–æ —Ç–µ–∫—Å—Ç–∞–º...\")\n",
        "\n",
        "import re\n",
        "for df in [train, test]:\n",
        "    df[\"reviews_exclamations\"] = df[\"all_reviews_text\"].fillna(\"\").astype(str).str.count(\"!\").astype(np.int32)\n",
        "    df[\"reviews_questions\"]   = df[\"all_reviews_text\"].fillna(\"\").astype(str).str.count(r\"\\?\").astype(np.int32)\n",
        "    lens = df[\"all_reviews_text\"].fillna(\"\").astype(str).str.len().replace(0, 1)\n",
        "    df[\"reviews_upper_frac\"]  = (df[\"all_reviews_text\"].fillna(\"\").astype(str).apply(lambda s: sum(c.isupper() for c in s)) / lens).astype(np.float32)\n",
        "\n",
        "def simple_clean(text: str) -> str:\n",
        "    s = str(text).lower()\n",
        "    s = re.sub(r'https?://\\S+|www\\.\\S+|\\S+@\\S+', ' ', s)\n",
        "    s = s.replace('—ë', '–µ')\n",
        "    s = re.sub(r'[^a-z–∞-—è0-9 ]+', ' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "train[\"reviews_clean\"] = train[\"all_reviews_text\"].fillna(\"\").astype(str).apply(simple_clean)\n",
        "test[\"reviews_clean\"]  = test[\"all_reviews_text\"].fillna(\"\").astype(str).apply(simple_clean)\n",
        "\n",
        "KW_PATTERNS = {\n",
        "    \"kw_pos\":       [r\"\\b–æ—Ç–ª–∏—á–Ω\\w*\", r\"\\b–∫–ª–∞—Å—Å–Ω\\w*\", r\"\\b—Ö–æ—Ä–æ—à\\w*\", r\"\\b–≤–∫—É—Å–Ω\\w*\", r\"\\b—Ä–µ–∫–æ–º–µ–Ω–¥\\w*\", r\"\\b–ª—é–±–∏–º\\w*\", r\"\\b–Ω—Ä–∞–≤\\w*\", r\"\\b—á–∏—Å—Ç\\w*\", r\"\\b—É–¥–æ–±–Ω\\w*\", r\"\\b–∫–æ–º—Ñ–æ—Ä—Ç\\w*\"],\n",
        "    \"kw_neg\":       [r\"\\b—É–∂–∞—Å\\w*\", r\"\\b–ø–ª–æ—Ö\\w*\", r\"\\b–Ω–µ–≤–∫—É—Å\\w*\", r\"\\b–≥—Ä—è–∑\\w*\", r\"\\b—Ö–∞–º\\w*\", r\"\\b–æ–±–º–∞–Ω\\w*\", r\"\\b—Ä–∞–∑–æ—á–∞—Ä\\w*\", r\"\\b–¥–æ—Ä–æ–≥\\w*\", r\"\\b–∫–æ—à–º–∞—Ä\\w*\"],\n",
        "    \"kw_service\":   [r\"\\b–≤–µ–∂–ª–∏–≤\\w*\", r\"\\b–ø–µ—Ä—Å–æ–Ω–∞–ª\\w*\", r\"\\b–æ—Ñ–∏—Ü–∏–∞–Ω—Ç\\w*\", r\"\\b–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä\\w*\", r\"\\b—Å–µ—Ä–≤–∏—Å\\w*\", r\"\\b–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç\\w*\", r\"\\b–æ—Ç–Ω–æ—à–µ–Ω\\w*\"],\n",
        "    \"kw_speed\":     [r\"\\b–±—ã—Å—Ç—Ä\\w*\", r\"\\b–º–µ–¥–ª–µ–Ω\\w*\", r\"\\b–¥–æ–ª–≥\\w*\", r\"\\b–æ—á–µ—Ä–µ–¥\\w*\", r\"\\b–æ–∂–∏–¥–∞–Ω\\w*\", r\"\\b–æ–ø–µ—Ä–∞—Ç–∏–≤\\w*\", r\"\\b–∑–∞—Ç—è–∂\\w*\"],\n",
        "    \"kw_price_pos\": [r\"\\b–¥–µ—à–µ–≤\\w*\", r\"\\b–≤—ã–≥–æ–¥\\w*\", r\"\\b–∞–∫—Ü–∏\\w*\", r\"\\b—Å–∫–∏–¥–∫\\w*\", r\"\\b–ª–æ—è–ª—å\\w*\", r\"\\b–±–æ–Ω—É—Å\\w*\"],\n",
        "    \"kw_price_neg\": [r\"\\b–¥–æ—Ä–æ–≥\\w*\", r\"\\b–∑–∞–≤—ã—à–µ–Ω–Ω\\w*\", r\"\\b—Ü–µ–Ω–Ω–∏–∫\\w*\", r\"\\b–ø–µ—Ä–µ–ø–ª–∞—Ç\\w*\", r\"\\b—Ü–µ–Ω\\w*\"],\n",
        "    \"kw_delivery\":  [r\"\\b–¥–æ—Å—Ç–∞–≤–∫\\w*\", r\"\\b–∫—É—Ä—å–µ—Ä\\w*\", r\"\\b–ø—Ä–∏–≤–µ–∑\\w*\", r\"\\b–æ–ø–∞–∑–¥\\w*\", r\"\\b—Å—Ä–æ–∫\\w*\", r\"\\b–∑–∞–∫–∞–∑\\w*\", r\"\\b–ø—Ä–∏–≤–æ–∑\\w*\"],\n",
        "    \"kw_quality\":   [r\"\\b–∫–∞—á–µ—Å—Ç–≤\\w*\", r\"\\b—Å–≤–µ–∂\\w*\", r\"\\b–≥–æ—Ä—è—á\\w*\", r\"\\b—Ö–æ–ª–æ–¥\\w*\", r\"\\b–ø—Ä–æ—Å—Ä–æ—á\\w*\", r\"\\b–±—Ä–∞–∫\\w*\", r\"\\b–∏—Å–ø—Ä–∞–≤–Ω\\w*\", r\"\\b–æ—Ä–∏–≥–∏–Ω–∞–ª\\w*\", r\"\\b–ø–æ–¥–¥–µ–ª\\w*\"],\n",
        "    \"kw_taste\":     [r\"\\b–≤–∫—É—Å–Ω\\w*\", r\"\\b—Å–æ–ª–µ–Ω\\w*\", r\"\\b–ø–µ—Ä–µ—Å–æ–ª–µ–Ω\\w*\", r\"\\b–ø–µ—Ä–µ—Å—É—à\\w*\", r\"\\b–æ—Å—Ç—Ä\\w*\", r\"\\b–∂–∏—Ä–Ω\\w*\", r\"\\b–ø–æ—Ä—Ü–∏\\w*\"],\n",
        "}\n",
        "KW_PATTERNS = {k: [re.compile(p, flags=re.U) for p in v] for k, v in KW_PATTERNS.items()}\n",
        "\n",
        "def kw_counts_regex(text: str):\n",
        "    toks = text.split()\n",
        "    total = max(1, len(toks))\n",
        "    feats = {}\n",
        "    for k, patterns in KW_PATTERNS.items():\n",
        "        cnt = 0\n",
        "        for pat in patterns:\n",
        "            cnt += len(pat.findall(text))\n",
        "        feats[f\"{k}_cnt\"] = int(cnt)\n",
        "        feats[f\"{k}_share\"] = float(cnt) / total\n",
        "    feats[\"reviews_tokens\"] = total\n",
        "    return feats\n",
        "\n",
        "def add_kw_simple(df):\n",
        "    feats_list = df[\"reviews_clean\"].fillna(\"\").apply(kw_counts_regex).tolist()\n",
        "    feats_df = pd.DataFrame(feats_list, index=df.index)\n",
        "    for c in feats_df.columns:\n",
        "        if feats_df[c].dtype == \"float64\": feats_df[c] = feats_df[c].astype(np.float32)\n",
        "        if feats_df[c].dtype == \"int64\":   feats_df[c] = feats_df[c].astype(np.int32)\n",
        "    return pd.concat([df, feats_df], axis=1)\n",
        "\n",
        "train = add_kw_simple(train)\n",
        "test = add_kw_simple(test)\n",
        "\n",
        "# —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–°–ü–ò–°–û–ö! —Ç.–∫. sklearn –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç set)\n",
        "RUSSIAN_STOPWORDS = [\n",
        "    \"–∏\",\"–≤\",\"–≤–æ\",\"–Ω–µ\",\"—á—Ç–æ\",\"–æ–Ω\",\"–æ–Ω–∞\",\"–æ–Ω–∏\",\"–æ–Ω–æ\",\"–∫–∞–∫\",\"–∫\",\"–∫–æ\",\"–Ω–∞\",\"–∏–∑\",\"–∑–∞\",\"–æ—Ç\",\"–¥–æ\",\"–ø–æ\",\"—Å\",\"—Å–æ\",\n",
        "    \"—É\",\"–æ\",\"–æ–±\",\"–æ–±–æ\",\"–ø—Ä–∏\",\"–¥–ª—è\",\"–±–µ–∑\",\"–ø—Ä–æ\",\"—ç—Ç–æ\",\"—ç—Ç–∞\",\"—ç—Ç–æ—Ç\",\"—ç—Ç–∏\",\"—Ç–æ—Ç\",\"—Ç–∞\",\"—Ç–µ\",\"–∂–µ\",\"–±—ã\",\"–ª–∏\",\n",
        "    \"—É–∂\",\"–≤–µ–¥—å\",\"–µ—â–µ\",\"—É–∂–µ\",\"–∫–æ–≥–¥–∞\",\"–≥–¥–µ\",\"–∫—É–¥–∞\",\"–æ—Ç–∫—É–¥–∞\",\"–ø–æ—Ç–æ–º—É\",\"–∫–æ—Ç–æ—Ä—ã–π\",\"–∫–æ—Ç–æ—Ä–∞—è\",\"–∫–æ—Ç–æ—Ä—ã–µ\",\"—Ç–∞–∫–∂–µ\",\n",
        "    \"—Ç–∞–∫\",\"—Ç—É—Ç\",\"—Ç–∞–º\",\"—Ç–æ–≥–¥–∞\",\"–ª–∏—à—å\",\"—Ç–æ–ª—å–∫–æ\",\"–æ—á–µ–Ω—å\",\"—Å–æ–≤—Å–µ–º\",\"–ø–æ—á—Ç–∏\",\"–µ—Å–ª–∏\",\"—Ç–æ\",\"–≤—Å–µ\",\"–≤–µ—Å—å\",\"–≤—Å—è\",\n",
        "    \"–º–æ–π\",\"–º–æ—è\",\"–º–æ–∏\",\"—Ç–≤–æ–π\",\"—Ç–≤–æ—è\",\"—Ç–≤–æ–∏\",\"–≤–∞—à\",\"–≤–∞—à–∞\",\"–≤–∞—à–∏\",\"–Ω–∞—à\",\"–Ω–∞—à–∞\",\"–Ω–∞—à–∏\",\"–∏—Ö\",\"—Å–≤–æ–π\",\"–∏–ª–∏\",\n",
        "    \"–ª–∏–±–æ\",\"–Ω–∏\",\"–¥–∞\",\"–Ω–µ—Ç\",\"–Ω—É\",\"–≤–æ—Ç\",\"–¥–∞–∂–µ\",\"—á—Ç–æ–±—ã\",\"—á—Ç–æ–±\",\"–º–µ–∂–¥—É\",\"–Ω–∞–¥\",\"–ø–æ–¥\",\"–ø–æ—Ç–æ–º\",\"–∑–∞—Ç–µ–º\",\"–æ–ø—è—Ç—å\",\n",
        "    \"—Å–Ω–æ–≤–∞\",\"–≤—Å–µ–≥–¥–∞\",\"–±—ã–ª\",\"–±—ã–ª–∞\",\"–±—ã–ª–æ\",\"–±—ã–ª–∏\",\"–±—ã—Ç—å\",\"–µ—Å—Ç—å\",\"–±—É–¥—É—Ç\",\"–±—É–¥–µ–º\",\"–Ω–µ—Ç—É\",\"–Ω–µ—Ç\",\"–µ—Å—Ç—å\"\n",
        "]\n",
        "\n",
        "# TF-IDF (word 1‚Äì2) + SVD\n",
        "all_reviews_clean = pd.concat([train['reviews_clean'], test['reviews_clean']], axis=0).tolist()\n",
        "vec_rev_word = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_features=100_000,\n",
        "    stop_words=RUSSIAN_STOPWORDS,  # –í–ê–ñ–ù–û: —Å–ø–∏—Å–æ–∫, –∞ –Ω–µ set\n",
        "    lowercase=False,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "X_rev_word = vec_rev_word.fit_transform(all_reviews_clean)\n",
        "svd_rev_word = TruncatedSVD(n_components=32, random_state=SEED)\n",
        "X_rev_word_svd = svd_rev_word.fit_transform(X_rev_word)\n",
        "revW_svd_cols = [f\"reviews_tfidfW_svd_{i}\" for i in range(X_rev_word_svd.shape[1])]\n",
        "revW_svd_train = pd.DataFrame(X_rev_word_svd[:len(train)], columns=revW_svd_cols, index=train.index).astype(np.float32)\n",
        "revW_svd_test  = pd.DataFrame(X_rev_word_svd[len(train):], columns=revW_svd_cols, index=test.index).astype(np.float32)\n",
        "del X_rev_word, X_rev_word_svd; gc.collect()\n",
        "\n",
        "# TF-IDF (char 3‚Äì5) + SVD\n",
        "vec_rev_char = TfidfVectorizer(\n",
        "    analyzer='char',\n",
        "    ngram_range=(3, 5),\n",
        "    min_df=2,\n",
        "    max_features=120_000,\n",
        "    lowercase=False,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "X_rev_char = vec_rev_char.fit_transform(all_reviews_clean)\n",
        "svd_rev_char = TruncatedSVD(n_components=24, random_state=SEED)\n",
        "X_rev_char_svd = svd_rev_char.fit_transform(X_rev_char)\n",
        "revC_svd_cols = [f\"reviews_tfidfC_svd_{i}\" for i in range(X_rev_char_svd.shape[1])]\n",
        "revC_svd_train = pd.DataFrame(X_rev_char_svd[:len(train)], columns=revC_svd_cols, index=train.index).astype(np.float32)\n",
        "revC_svd_test  = pd.DataFrame(X_rev_char_svd[len(train):], columns=revC_svd_cols, index=test.index).astype(np.float32)\n",
        "del X_rev_char, X_rev_char_svd; gc.collect()\n",
        "\n",
        "print(f\"‚úÖ EVR (reviews word): {svd_rev_word.explained_variance_ratio_.sum():.3f}\")\n",
        "print(f\"‚úÖ EVR (reviews char): {svd_rev_char.explained_variance_ratio_.sum():.3f}\")\n",
        "\n",
        "# =============== –¢–ï–ö–°–¢-–°–¢–ê–¢–´ ===============\n",
        "for df in [train, test]:\n",
        "    df[\"name_len\"] = np.log1p(df[\"name\"].fillna(\"\").str.len().astype(float)).astype(np.float32)\n",
        "    df[\"reviews_text_len\"] = np.log1p(df[\"all_reviews_text\"].fillna(\"\").str.len().astype(float)).astype(np.float32)\n",
        "\n",
        "# =============== BRAND –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + —Ä–∞–∑–º–µ—Ä —Å–µ—Ç–∏ ===============\n",
        "import re as _re\n",
        "def normalize_brand(x: str) -> str:\n",
        "    s = str(x).lower()\n",
        "    s = _re.sub(r'[^a-z–∞-—è0-9 ]+', ' ', s)\n",
        "    s = _re.sub(r'\\b(–æ–æ–æ|–∏–ø|ooo|oao|zao|–º–∞–≥–∞–∑–∏–Ω|–∫–∞—Ñ–µ|–±–∞—Ä|–∞–ø—Ç–µ–∫–∞|—Å–∞–ª–æ–Ω|clinic|salon|restaurant|rest|grill)\\b', ' ', s)\n",
        "    s = _re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "for df in [train, test]:\n",
        "    df['brand'] = df['name'].map(normalize_brand)\n",
        "\n",
        "brand_size = pd.concat([train['brand'], test['brand']]).value_counts()\n",
        "for df in [train, test]:\n",
        "    df['brand_size'] = df['brand'].map(brand_size).fillna(1).astype(np.int32)\n",
        "\n",
        "# =============== OOF TARGET ENCODING (category, geo_cluster, brand) ===============\n",
        "print(\"üéØ Target encoding (OOF) –ø–æ category/geo/brand...\")\n",
        "te_cat_tr, te_cat_te = oof_target_encoding(train, test, col=\"category\", target_col=\"target\",\n",
        "                                           n_splits=5, groups=train[\"geo_cluster\"], smoothing=5.0, global_mean=global_mean)\n",
        "te_geo_tr, te_geo_te = oof_target_encoding(train, test, col=\"geo_cluster\", target_col=\"target\",\n",
        "                                           n_splits=5, groups=train[\"geo_cluster\"], smoothing=10.0, global_mean=global_mean)\n",
        "te_brand_tr, te_brand_te = oof_target_encoding(train, test, col=\"brand\", target_col=\"target\",\n",
        "                                               n_splits=5, groups=train[\"geo_cluster\"], smoothing=5.0, global_mean=global_mean)\n",
        "\n",
        "train[\"category_te\"] = te_cat_tr; test[\"category_te\"] = te_cat_te\n",
        "train[\"geo_cluster_te\"] = te_geo_tr; test[\"geo_cluster_te\"] = te_geo_te\n",
        "train[\"brand_te\"] = te_brand_tr; test[\"brand_te\"] = te_brand_te\n",
        "\n",
        "# =============== KNN OOF –ì–ï–û-–§–ò–ß–ò (0.5 –∏ 1.0 –∫–º; all/same_cat + IDW + HHI) ===============\n",
        "print(\"üó∫Ô∏è KNN OOF –≥–µ–æ-—Ñ–∏—á–∏ (0.5/1.0 –∫–º)...\")\n",
        "EARTH_R = 6371.0\n",
        "\n",
        "def knn_geo_oof_features_simple(train_df, test_df, radii_km=(0.5, 1.0), idw_eps=0.05):\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "    def build_feats(n, radii):\n",
        "        feats = {}\n",
        "        for r in radii:\n",
        "            feats[f\"knn_count_all_{r}km\"] = np.zeros(n, dtype=np.float32)\n",
        "            feats[f\"knn_mean_all_{r}km\"] = np.zeros(n, dtype=np.float32)\n",
        "            feats[f\"knn_idw_mean_all_{r}km\"] = np.zeros(n, dtype=np.float32)\n",
        "            feats[f\"knn_count_same_{r}km\"] = np.zeros(n, dtype=np.float32)\n",
        "            feats[f\"knn_mean_same_{r}km\"] = np.zeros(n, dtype=np.float32)\n",
        "            feats[f\"knn_idw_mean_same_{r}km\"] = np.zeros(n, dtype=np.float32)\n",
        "            feats[f\"knn_ratio_same_all_{r}km\"] = np.zeros(n, dtype=np.float32)\n",
        "        feats[\"knn_hhi_1km\"] = np.zeros(n, dtype=np.float32)\n",
        "        return feats\n",
        "\n",
        "    # OOF\n",
        "    oof_feats = build_feats(len(train_df), radii_km)\n",
        "    for trn_idx, val_idx in gkf.split(train_df, groups=train_df[\"geo_cluster\"]):\n",
        "        trn, val = train_df.iloc[trn_idx], train_df.iloc[val_idx]\n",
        "        tree = BallTree(np.c_[trn[\"lat_rad\"], trn[\"lon_rad\"]], metric='haversine')\n",
        "        for r in radii_km:\n",
        "            ind, dist = tree.query_radius(np.c_[val[\"lat_rad\"], val[\"lon_rad\"]], r=r/EARTH_R, return_distance=True, sort_results=True)\n",
        "            all_means, all_counts, idw_all = [], [], []\n",
        "            same_means, same_counts, idw_same = [], [], []\n",
        "            hhis = []\n",
        "            for i, (nbr_idx, nbr_dist) in enumerate(zip(ind, dist)):\n",
        "                if len(nbr_idx) == 0:\n",
        "                    all_counts.append(0); all_means.append(global_mean); idw_all.append(global_mean)\n",
        "                    same_counts.append(0); same_means.append(global_mean); idw_same.append(global_mean)\n",
        "                    if r == 1.0: hhis.append(0.0)\n",
        "                    continue\n",
        "                nbr_ids = trn.index.values[nbr_idx]\n",
        "                nbr_vals = train_df.loc[nbr_ids, \"target\"].values\n",
        "                all_counts.append(len(nbr_ids))\n",
        "                all_means.append(np.mean(nbr_vals))\n",
        "                dk = (nbr_dist * EARTH_R) + idw_eps\n",
        "                w = 1.0 / dk\n",
        "                idw_all.append(np.average(nbr_vals, weights=w))\n",
        "                mask_same = (train_df.loc[nbr_ids, \"category\"].values == val.iloc[i][\"category\"])\n",
        "                if mask_same.any():\n",
        "                    v_same = nbr_vals[mask_same]\n",
        "                    same_counts.append(len(v_same))\n",
        "                    same_means.append(np.mean(v_same))\n",
        "                    idw_same.append(np.average(v_same, weights=w[mask_same]))\n",
        "                else:\n",
        "                    same_counts.append(0); same_means.append(global_mean); idw_same.append(global_mean)\n",
        "                if r == 1.0:\n",
        "                    from collections import Counter as Ctr\n",
        "                    cats = train_df.loc[nbr_ids, \"category\"].values\n",
        "                    n = len(cats)\n",
        "                    if n > 0:\n",
        "                        ctr = Ctr(cats)\n",
        "                        shares = np.array([c/n for c in ctr.values()], dtype=np.float32)\n",
        "                        hhis.append(float(np.sum(shares**2)))\n",
        "                    else:\n",
        "                        hhis.append(0.0)\n",
        "            oof_feats[f\"knn_count_all_{r}km\"][val_idx]    = np.array(all_counts, dtype=np.float32)\n",
        "            oof_feats[f\"knn_mean_all_{r}km\"][val_idx]     = np.array(all_means, dtype=np.float32)\n",
        "            oof_feats[f\"knn_idw_mean_all_{r}km\"][val_idx] = np.array(idw_all, dtype=np.float32)\n",
        "            oof_feats[f\"knn_count_same_{r}km\"][val_idx]    = np.array(same_counts, dtype=np.float32)\n",
        "            oof_feats[f\"knn_mean_same_{r}km\"][val_idx]     = np.array(same_means, dtype=np.float32)\n",
        "            oof_feats[f\"knn_idw_mean_same_{r}km\"][val_idx] = np.array(idw_same, dtype=np.float32)\n",
        "            ratio = (oof_feats[f\"knn_count_same_{r}km\"][val_idx] + 1.0) / (oof_feats[f\"knn_count_all_{r}km\"][val_idx] + 1.0)\n",
        "            oof_feats[f\"knn_ratio_same_all_{r}km\"][val_idx] = ratio.astype(np.float32)\n",
        "            if r == 1.0:\n",
        "                oof_feats[\"knn_hhi_1km\"][val_idx] = np.array(hhis, dtype=np.float32)\n",
        "\n",
        "    # test\n",
        "    test_feats = build_feats(len(test_df), radii_km)\n",
        "    tree_full = BallTree(np.c_[train_df[\"lat_rad\"], train_df[\"lon_rad\"]], metric='haversine')\n",
        "    for r in radii_km:\n",
        "        ind, dist = tree_full.query_radius(np.c_[test_df[\"lat_rad\"], test_df[\"lon_rad\"]], r=r/EARTH_R, return_distance=True, sort_results=True)\n",
        "        all_means, all_counts, idw_all = [], [], []\n",
        "        same_means, same_counts, idw_same = [], [], []\n",
        "        hhis = []\n",
        "        for i, (nbr_idx, nbr_dist) in enumerate(zip(ind, dist)):\n",
        "            if len(nbr_idx) == 0:\n",
        "                all_counts.append(0); all_means.append(global_mean); idw_all.append(global_mean)\n",
        "                same_counts.append(0); same_means.append(global_mean); idw_same.append(global_mean)\n",
        "                if r == 1.0: hhis.append(0.0)\n",
        "                continue\n",
        "            nbr_ids = train_df.index.values[nbr_idx]\n",
        "            nbr_vals = train_df.loc[nbr_ids, \"target\"].values\n",
        "            all_counts.append(len(nbr_ids))\n",
        "            all_means.append(np.mean(nbr_vals))\n",
        "            dk = (nbr_dist * EARTH_R) + idw_eps\n",
        "            w = 1.0 / dk\n",
        "            idw_all.append(np.average(nbr_vals, weights=w))\n",
        "            mask_same = (train_df.loc[nbr_ids, \"category\"].values == test_df.iloc[i][\"category\"])\n",
        "            if mask_same.any():\n",
        "                v_same = nbr_vals[mask_same]\n",
        "                same_counts.append(len(v_same))\n",
        "                same_means.append(np.mean(v_same))\n",
        "                idw_same.append(np.average(v_same, weights=w[mask_same]))\n",
        "            else:\n",
        "                same_counts.append(0); same_means.append(global_mean); idw_same.append(global_mean)\n",
        "            if r == 1.0:\n",
        "                from collections import Counter as Ctr\n",
        "                cats = train_df.loc[nbr_ids, \"category\"].values\n",
        "                n = len(cats)\n",
        "                if n > 0:\n",
        "                    ctr = Ctr(cats)\n",
        "                    shares = np.array([c/n for c in ctr.values()], dtype=np.float32)\n",
        "                    hhis.append(float(np.sum(shares**2)))\n",
        "                else:\n",
        "                    hhis.append(0.0)\n",
        "        test_feats[f\"knn_count_all_{r}km\"]    = np.array(all_counts, dtype=np.float32)\n",
        "        test_feats[f\"knn_mean_all_{r}km\"]     = np.array(all_means, dtype=np.float32)\n",
        "        test_feats[f\"knn_idw_mean_all_{r}km\"] = np.array(idw_all, dtype=np.float32)\n",
        "        test_feats[f\"knn_count_same_{r}km\"]    = np.array(same_counts, dtype=np.float32)\n",
        "        test_feats[f\"knn_mean_same_{r}km\"]     = np.array(same_means, dtype=np.float32)\n",
        "        test_feats[f\"knn_idw_mean_same_{r}km\"] = np.array(idw_same, dtype=np.float32)\n",
        "        ratio = (test_feats[f\"knn_count_same_{r}km\"] + 1.0) / (test_feats[f\"knn_count_all_{r}km\"] + 1.0)\n",
        "        test_feats[f\"knn_ratio_same_all_{r}km\"] = ratio.astype(np.float32)\n",
        "        if r == 1.0:\n",
        "            test_feats[\"knn_hhi_1km\"] = np.array(hhis, dtype=np.float32)\n",
        "\n",
        "    return oof_feats, test_feats\n",
        "\n",
        "knn_oof_tr, knn_te = knn_geo_oof_features_simple(train, test, radii_km=(0.5, 1.0))\n",
        "for k, v in knn_oof_tr.items(): train[k] = v\n",
        "for k, v in knn_te.items():     test[k] = v\n",
        "\n",
        "# =============== DEMO ALIGNMENT (OOF) + –≠–ù–¢–†–û–ü–ò–ò ===============\n",
        "print(\"üß¨ Demo alignment + —ç–Ω—Ç—Ä–æ–ø–∏–∏ –¥–µ–º–æ–≥—Ä–∞—Ñ–∏–∏...\")\n",
        "def build_demo_matrix(df):\n",
        "    eps = 1e-9\n",
        "    cols = df.columns\n",
        "    age_cols = [c for c in cols if c.startswith(\"age_\") and c.endswith(\"_1000m\")]\n",
        "    age_sum = df[age_cols].astype(float).sum(axis=1) + eps if len(age_cols)>0 else pd.Series(1.0, index=df.index)\n",
        "    age_shares = {}\n",
        "    for c in age_cols:\n",
        "        age_shares[c+\"_share\"] = (df[c].astype(float) / age_sum).astype(np.float32)\n",
        "    if \"female_1000m\" in cols and \"male_1000m\" in cols:\n",
        "        sex_sum = df[\"female_1000m\"].astype(float) + df[\"male_1000m\"].astype(float) + eps\n",
        "        female_share = (df[\"female_1000m\"].astype(float) / sex_sum).astype(np.float32).rename(\"female_share\")\n",
        "    else:\n",
        "        female_share = pd.Series(0.5, index=df.index, name=\"female_share\")\n",
        "    if \"married_1000m\" in cols and \"not_married_1000m\" in cols:\n",
        "        sm = df[\"married_1000m\"].astype(float) + df[\"not_married_1000m\"].astype(float) + eps\n",
        "        married_share = (df[\"married_1000m\"].astype(float) / sm).astype(np.float32).rename(\"married_share\")\n",
        "    else:\n",
        "        married_share = pd.Series(0.5, index=df.index, name=\"married_share\")\n",
        "    if \"has_children_1000m\" in cols and \"no_children_1000m\" in cols:\n",
        "        sm = df[\"has_children_1000m\"].astype(float) + df[\"no_children_1000m\"].astype(float) + eps\n",
        "        has_children_share = (df[\"has_children_1000m\"].astype(float) / sm).astype(np.float32).rename(\"has_children_share\")\n",
        "    else:\n",
        "        has_children_share = pd.Series(0.5, index=df.index, name=\"has_children_share\")\n",
        "    if \"employed_1000m\" in cols and \"unemployed_1000m\" in cols:\n",
        "        sm = df[\"employed_1000m\"].astype(float) + df[\"unemployed_1000m\"].astype(float) + eps\n",
        "        employed_share = (df[\"employed_1000m\"].astype(float) / sm).astype(np.float32).rename(\"employed_share\")\n",
        "    else:\n",
        "        employed_share = pd.Series(0.5, index=df.index, name=\"employed_share\")\n",
        "    if \"higher_education_1000m\" in cols and \"no_higher_education_1000m\" in cols:\n",
        "        sm = df[\"higher_education_1000m\"].astype(float) + df[\"no_higher_education_1000m\"].astype(float) + eps\n",
        "        higher_edu_share = (df[\"higher_education_1000m\"].astype(float) / sm).astype(np.float32).rename(\"higher_edu_share\")\n",
        "    else:\n",
        "        higher_edu_share = pd.Series(0.5, index=df.index, name=\"higher_edu_share\")\n",
        "    parts = [pd.DataFrame(age_shares)] if len(age_shares) > 0 else []\n",
        "    parts += [female_share, married_share, has_children_share, employed_share, higher_edu_share]\n",
        "    demo_df = pd.concat(parts, axis=1)\n",
        "    demo_df = demo_df.fillna(demo_df.median())\n",
        "    return demo_df\n",
        "\n",
        "def demo_alignment_oof(train_df, test_df, category_col=\"category\"):\n",
        "    demo_train = build_demo_matrix(train_df)\n",
        "    demo_test = build_demo_matrix(test_df)\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "    align_train = np.zeros(len(train_df), dtype=np.float32)\n",
        "    for trn_idx, val_idx in gkf.split(train_df, groups=train_df[\"geo_cluster\"]):\n",
        "        trn, val = train_df.iloc[trn_idx], train_df.iloc[val_idx]\n",
        "        demo_trn, demo_val = demo_train.iloc[trn_idx], demo_train.iloc[val_idx]\n",
        "        protos = demo_trn.groupby(trn[category_col]).mean()\n",
        "        cur = demo_val.values\n",
        "        proto_mat = protos.reindex(val[category_col]).values\n",
        "        cur_n = cur / (np.linalg.norm(cur, axis=1, keepdims=True) + 1e-9)\n",
        "        proto_n = proto_mat / (np.linalg.norm(proto_mat, axis=1, keepdims=True) + 1e-9)\n",
        "        sim = np.sum(cur_n * proto_n, axis=1)\n",
        "        global_proto = demo_trn.mean().values\n",
        "        global_proto_n = global_proto / (np.linalg.norm(global_proto) + 1e-9)\n",
        "        mask_nan = np.isnan(sim)\n",
        "        if mask_nan.any():\n",
        "            sim[mask_nan] = np.sum(cur_n[mask_nan] * global_proto_n, axis=1)\n",
        "        align_train[val_idx] = sim.astype(np.float32)\n",
        "    protos_full = build_demo_matrix(train_df).groupby(train_df[category_col]).mean()\n",
        "    cur = demo_test.values\n",
        "    proto_mat = protos_full.reindex(test_df[category_col]).values\n",
        "    cur_n = cur / (np.linalg.norm(cur, axis=1, keepdims=True) + 1e-9)\n",
        "    proto_n = proto_mat / (np.linalg.norm(proto_mat, axis=1, keepdims=True) + 1e-9)\n",
        "    sim_te = np.sum(cur_n * proto_n, axis=1)\n",
        "    global_proto = build_demo_matrix(train_df).mean().values\n",
        "    global_proto_n = global_proto / (np.linalg.norm(global_proto) + 1e-9)\n",
        "    mask_nan = np.isnan(sim_te)\n",
        "    if mask_nan.any():\n",
        "        sim_te[mask_nan] = np.sum(cur_n[mask_nan] * global_proto_n, axis=1)\n",
        "    align_test = sim_te.astype(np.float32)\n",
        "    return align_train, align_test\n",
        "\n",
        "demo_align_tr, demo_align_te = demo_alignment_oof(train, test, category_col=\"category\")\n",
        "train[\"demo_alignment\"] = demo_align_tr.astype(np.float32)\n",
        "test[\"demo_alignment\"] = demo_align_te.astype(np.float32)\n",
        "\n",
        "def entropy_from_cols(df, cols):\n",
        "    eps = 1e-9\n",
        "    if len(cols) == 0:\n",
        "        return np.zeros(len(df), dtype=np.float32)\n",
        "    arr = df[cols].astype(float).values\n",
        "    s = arr.sum(axis=1, keepdims=True) + eps\n",
        "    p = arr / s\n",
        "    ent = -np.sum(p * np.log(p + eps), axis=1)\n",
        "    return ent.astype(np.float32)\n",
        "\n",
        "age_cols_1000 = [c for c in train.columns if c.startswith(\"age_\") and c.endswith(\"_1000m\")]\n",
        "income_cols_1000 = [c for c in train.columns if c.endswith(\"_income_1000m\")]\n",
        "\n",
        "for df in [train, test]:\n",
        "    df[\"age_entropy_1000m\"] = entropy_from_cols(df, age_cols_1000)\n",
        "    df[\"income_entropy_1000m\"] = entropy_from_cols(df, income_cols_1000)\n",
        "\n",
        "# =============== –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –¢–ï–ö–°–¢–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í + –ò–ù–¢–ï–†–ê–ö–¶–ò–ò ===============\n",
        "print(\"üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∏—á...\")\n",
        "train = pd.concat([train, name_svd_train, revW_svd_train, revC_svd_train], axis=1)\n",
        "test  = pd.concat([test,  name_svd_test,  revW_svd_test,  revC_svd_test], axis=1)\n",
        "del name_svd_train, name_svd_test, revW_svd_train, revW_svd_test, revC_svd_train, revC_svd_test\n",
        "gc.collect()\n",
        "\n",
        "for df in [train, test]:\n",
        "    df[\"inv_dist\"] = (1.0 / (1.0 + df[\"dist_to_center_km\"])).astype(np.float32)\n",
        "    for base in [\"name_tfidf_svd_\", \"reviews_tfidfW_svd_\", \"reviews_tfidfC_svd_\"]:\n",
        "        for i in range(3):\n",
        "            col = f\"{base}{i}\"\n",
        "            if col in df.columns:\n",
        "                df[f\"{col}_x_rcount\"]  = (df[col] * df[\"review_count\"]).astype(np.float32)\n",
        "                df[f\"{col}_x_catTE\"]   = (df[col] * df[\"category_te\"]).astype(np.float32)\n",
        "                df[f\"{col}_x_invDist\"] = (df[col] * df[\"inv_dist\"]).astype(np.float32)\n",
        "\n",
        "# =============== –¢–†–ê–ù–°–§–û–†–ú –¢–ê–†–ì–ï–¢–ê ===============\n",
        "print(\"üìä –ö–≤–∞–Ω—Ç–∏–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ target...\")\n",
        "qt = QuantileTransformer(output_distribution=\"normal\", random_state=SEED, n_quantiles=min(2000, max(10, len(train)//5)))\n",
        "train[\"target_qt\"] = qt.fit_transform(train[[\"target\"]]).astype(np.float32)\n",
        "\n",
        "# =============== –ü–†–ò–ó–ù–ê–ö–ò: —Å–ø–∏—Å–æ–∫ –∏ —Ç–∏–ø—ã ===============\n",
        "ignore_cols = {\n",
        "    \"id\", \"name\", \"category\", \"address\", \"coordinates\", \"brand\",\n",
        "    \"all_reviews_text\", \"reviews_clean\", \"target\", \"target_qt\",\n",
        "    \"tile_500m\", \"tile_1000m\", \"tile_500m_x_category\", \"tile_1000m_x_category\"\n",
        "}\n",
        "num_cols = [c for c in train.columns if c not in ignore_cols]\n",
        "\n",
        "for df in [train, test]:\n",
        "    for c in num_cols:\n",
        "        if c in df.columns:\n",
        "            if df[c].dtype == \"float64\":\n",
        "                df[c] = df[c].astype(np.float32)\n",
        "            if df[c].dtype == \"int64\":\n",
        "                df[c] = df[c].astype(np.int32)\n",
        "\n",
        "available_features = [c for c in num_cols if c in test.columns]\n",
        "print(f\"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(available_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "tfidf_features_count = sum(1 for f in available_features if f.startswith(\"name_tfidf_svd_\") or f.startswith(\"reviews_tfidfW_svd_\") or f.startswith(\"reviews_tfidfC_svd_\"))\n",
        "print(f\" - TF-IDF SVD –ø—Ä–∏–∑–Ω–∞–∫–∏: {tfidf_features_count}\")\n",
        "\n",
        "# =============== LightGBM (CV) + –º–æ–Ω–æ—Ç–æ–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è ===============\n",
        "params = {\n",
        "    \"objective\": \"regression\",\n",
        "    \"metric\": \"mae\",\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 127,\n",
        "    \"feature_fraction\": 0.8,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 5,\n",
        "    \"lambda_l1\": 0.2,\n",
        "    \"lambda_l2\": 0.3,\n",
        "    \"min_data_in_leaf\": 20,\n",
        "    \"max_depth\": -1,\n",
        "    \"n_estimators\": 400,\n",
        "    \"random_state\": SEED,\n",
        "    \"verbosity\": -1,\n",
        "    \"n_jobs\": -1\n",
        "}\n",
        "\n",
        "monotone_map = {\n",
        "    'review_count': +1,\n",
        "    'traffic_300m': +1,\n",
        "    'traffic_1000m': +1,\n",
        "    'dist_to_center_km': -1,\n",
        "    'inv_dist': +1,\n",
        "    'knn_count_all_0.5km': +1,\n",
        "    'knn_count_all_1.0km': +1\n",
        "}\n",
        "constraints = [monotone_map.get(f, 0) for f in available_features]\n",
        "params[\"monotone_constraints\"] = constraints\n",
        "\n",
        "folds = GroupKFold(n_splits=5)\n",
        "oof_predictions = np.zeros(len(train), dtype=np.float32)\n",
        "best_iterations = []\n",
        "\n",
        "print(\"\\nüéØ –û–±—É—á–µ–Ω–∏–µ LightGBM (CV)...\")\n",
        "for fold, (trn_idx, val_idx) in enumerate(folds.split(train, train[\"target_qt\"], groups=train[\"geo_cluster\"])):\n",
        "    print(f\"Fold {fold+1}/5\")\n",
        "    X_tr, y_tr = train.iloc[trn_idx][available_features], train.iloc[trn_idx][\"target_qt\"]\n",
        "    X_val, y_val = train.iloc[val_idx][available_features], train.iloc[val_idx][\"target_qt\"]\n",
        "\n",
        "    model = lgb.LGBMRegressor(**params)\n",
        "    try:\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_metric=\"mae\",\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
        "                lgb.log_evaluation(period=100)\n",
        "            ]\n",
        "        )\n",
        "    except lgb.basic.LightGBMError:\n",
        "        print(\"‚ö†Ô∏è Monotone constraints –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω—ã. –û–±—É—á–∞–µ–º –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π.\")\n",
        "        p2 = {**params}\n",
        "        p2.pop(\"monotone_constraints\", None)\n",
        "        model = lgb.LGBMRegressor(**p2)\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_metric=\"mae\",\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
        "                lgb.log_evaluation(period=100)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    preds_val = model.predict(X_val, num_iteration=model.best_iteration_)\n",
        "    oof_predictions[val_idx] = preds_val.astype(np.float32)\n",
        "    best_iterations.append(model.best_iteration_)\n",
        "\n",
        "    fold_mae = mean_absolute_error(\n",
        "        qt.inverse_transform(y_val.values.reshape(-1, 1)).ravel(),\n",
        "        qt.inverse_transform(preds_val.reshape(-1, 1)).ravel()\n",
        "    )\n",
        "    print(f\"  Fold MAE: {fold_mae:.4f}\")\n",
        "\n",
        "oof_pred = qt.inverse_transform(oof_predictions.reshape(-1, 1)).ravel()\n",
        "oof_mae = mean_absolute_error(train[\"target\"].values, oof_pred)\n",
        "print(f\"\\nüéØ Final OOF MAE (–¥–æ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏): {oof_mae:.4f}\")\n",
        "\n",
        "# =============== –ò–ó–û–¢–û–ù–ò–ß–ï–°–ö–ê–Ø –ö–ê–õ–ò–ë–†–û–í–ö–ê –ü–û OOF ===============\n",
        "print(\"üìà –ò–∑–æ—Ç–æ–Ω–∏—á–µ—Å–∫–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø–æ OOF...\")\n",
        "iso = IsotonicRegression(out_of_bounds='clip')\n",
        "iso.fit(oof_pred, train['target'].values)\n",
        "oof_pred_cal = iso.transform(oof_pred)\n",
        "oof_mae_cal = mean_absolute_error(train[\"target\"].values, np.clip(oof_pred_cal, 1.0, 5.0))\n",
        "print(f\"üéØ OOF MAE –ø–æ—Å–ª–µ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏: {oof_mae_cal:.4f}\")\n",
        "\n",
        "# =============== –§–ò–ù–ê–õ–¨–ù–ê–Ø –ú–û–î–ï–õ–¨ ===============\n",
        "print(\"\\nüöÄ –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏...\")\n",
        "final_n_estimators = int(np.clip(np.mean(best_iterations) * 1.1, 200, 1000))\n",
        "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º n_estimators={final_n_estimators}\")\n",
        "\n",
        "final_model = lgb.LGBMRegressor(**{**params, \"n_estimators\": final_n_estimators})\n",
        "final_model.fit(train[available_features], train[\"target_qt\"])\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': available_features,\n",
        "    'importance': final_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nüìä –¢–æ–ø-20 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "print(feature_importance.head(20))\n",
        "tfidf_in_top = sum(feature_importance.head(20)['feature'].str.contains('tfidf'))\n",
        "print(f\"üìù TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ç–æ–ø-20: {tfidf_in_top}\")\n",
        "\n",
        "# =============== –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï + –ö–ê–õ–ò–ë–†–û–í–ö–ê ===============\n",
        "print(\"\\nüßÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–µ...\")\n",
        "test_pred_qt = final_model.predict(test[available_features])\n",
        "test_pred = qt.inverse_transform(test_pred_qt.reshape(-1, 1)).ravel()\n",
        "test_pred = iso.transform(test_pred)\n",
        "test_pred = np.clip(test_pred, 1.0, 5.0)\n",
        "\n",
        "# =============== –°–û–•–†–ê–ù–ï–ù–ò–ï ===============\n",
        "sub = pd.DataFrame({\"id\": test[\"id\"], \"target\": test_pred})\n",
        "sub.to_csv(\"submission_tfidf_geo_plus.csv\", index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ submission_tfidf_geo_plus.csv –≥–æ—Ç–æ–≤!\")\n",
        "print(f\"üéØ Final OOF MAE (raw): {oof_mae:.4f} | (calibrated): {oof_mae_cal:.4f}\")\n",
        "print(f\"üîß –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {len(available_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzonYPWy_5dj",
        "outputId": "73482265-c5d0-4376-a46d-ddfafb6e2663"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
            "–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä train: 41105\n",
            "–†–∞–∑–º–µ—Ä test: 9276\n",
            "–†–∞–∑–º–µ—Ä reviews: 440082\n",
            "üßπ –£–¥–∞–ª—è–µ–º –æ–±—ä–µ–∫—Ç—ã —Å target=0 –∏–∑ train...\n",
            "–£–¥–∞–ª–µ–Ω–æ: 3938 | –ù–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä train: 37167\n",
            "üìä –ê–≥–≥—Ä–µ–≥–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ –ø–æ id...\n",
            "üìç –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –±–∞–∑–æ–≤–∞—è –≥–µ–æ–≥—Ä–∞—Ñ–∏—è...\n",
            "üß≠ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≥–µ–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏...\n",
            "üîé –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ 140 –ø–∞—Ä *_300m/_1000m.\n",
            "üß± –ì–µ–æ-—Ç–∞–π–ª—ã 500–º/1000–º –∏ OOF-TE...\n",
            "üî§ TF-IDF –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º (char 3‚Äì5) + SVD...\n",
            "‚úÖ EVR (name): 0.096\n",
            "üßº –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∑—ã–≤–æ–≤ –∏ TF-IDF –ø–æ —Ç–µ–∫—Å—Ç–∞–º...\n",
            "‚úÖ EVR (reviews word): 0.055\n",
            "‚úÖ EVR (reviews char): 0.118\n",
            "üéØ Target encoding (OOF) –ø–æ category/geo/brand...\n",
            "üó∫Ô∏è KNN OOF –≥–µ–æ-—Ñ–∏—á–∏ (0.5/1.0 –∫–º)...\n",
            "üß¨ Demo alignment + —ç–Ω—Ç—Ä–æ–ø–∏–∏ –¥–µ–º–æ–≥—Ä–∞—Ñ–∏–∏...\n",
            "üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∏—á...\n",
            "üìä –ö–≤–∞–Ω—Ç–∏–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ target...\n",
            "üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 1005 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
            " - TF-IDF SVD –ø—Ä–∏–∑–Ω–∞–∫–∏: 99\n",
            "\n",
            "üéØ –û–±—É—á–µ–Ω–∏–µ LightGBM (CV)...\n",
            "Fold 1/5\n",
            "[100]\tvalid_0's l1: 0.554883\n",
            "[200]\tvalid_0's l1: 0.551798\n",
            "[300]\tvalid_0's l1: 0.554204\n",
            "  Fold MAE: 0.2250\n",
            "Fold 2/5\n",
            "[100]\tvalid_0's l1: 0.565558\n",
            "[200]\tvalid_0's l1: 0.56591\n",
            "[300]\tvalid_0's l1: 0.567218\n",
            "  Fold MAE: 0.2344\n",
            "Fold 3/5\n",
            "[100]\tvalid_0's l1: 0.563897\n",
            "[200]\tvalid_0's l1: 0.561393\n",
            "[300]\tvalid_0's l1: 0.56113\n",
            "[400]\tvalid_0's l1: 0.562198\n",
            "  Fold MAE: 0.2360\n",
            "Fold 4/5\n",
            "[100]\tvalid_0's l1: 0.559719\n",
            "[200]\tvalid_0's l1: 0.557845\n",
            "[300]\tvalid_0's l1: 0.558496\n",
            "  Fold MAE: 0.2365\n",
            "Fold 5/5\n",
            "[100]\tvalid_0's l1: 0.561919\n",
            "[200]\tvalid_0's l1: 0.561523\n",
            "[300]\tvalid_0's l1: 0.561415\n",
            "[400]\tvalid_0's l1: 0.562071\n",
            "  Fold MAE: 0.2359\n",
            "\n",
            "üéØ Final OOF MAE (–¥–æ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏): 0.2336\n",
            "üìà –ò–∑–æ—Ç–æ–Ω–∏—á–µ—Å–∫–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø–æ OOF...\n",
            "üéØ OOF MAE –ø–æ—Å–ª–µ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏: 0.2382\n",
            "\n",
            "üöÄ –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏...\n",
            "–ò—Å–ø–æ–ª—å–∑—É–µ–º n_estimators=221\n",
            "\n",
            "üìä –¢–æ–ø-20 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
            "                           feature  importance\n",
            "886                       brand_te         428\n",
            "884                    category_te         410\n",
            "880                 reviews_tokens         251\n",
            "946          reviews_tfidfW_svd_25         243\n",
            "863                   kw_pos_share         241\n",
            "881                       name_len         235\n",
            "949          reviews_tfidfW_svd_28         226\n",
            "882               reviews_text_len         215\n",
            "960           reviews_tfidfC_svd_7         202\n",
            "943          reviews_tfidfW_svd_22         201\n",
            "959           reviews_tfidfC_svd_6         194\n",
            "902                 demo_alignment         193\n",
            "294         dist_to_cluster_center         193\n",
            "951          reviews_tfidfW_svd_30         193\n",
            "956           reviews_tfidfC_svd_3         190\n",
            "932          reviews_tfidfW_svd_11         185\n",
            "996  reviews_tfidfC_svd_0_x_rcount         184\n",
            "965          reviews_tfidfC_svd_12         180\n",
            "919              name_tfidf_svd_14         177\n",
            "914               name_tfidf_svd_9         172\n",
            "üìù TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ç–æ–ø-20: 12\n",
            "\n",
            "üßÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–µ...\n",
            "\n",
            "‚úÖ submission_tfidf_geo_plus.csv –≥–æ—Ç–æ–≤!\n",
            "üéØ Final OOF MAE (raw): 0.2336 | (calibrated): 0.2382\n",
            "üîß –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ 1005 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n"
          ]
        }
      ]
    }
  ]
}