{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7cgjGRlJIz_",
        "outputId": "5f499bbc-2d60-4e74-caa0-7e707a60cd3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, get_linear_schedule_with_warmup\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import warnings\n",
        "from torch.optim import AdamW\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================\n",
        "class Config:\n",
        "    model_name = \"sberbank-ai/ruBert-base\"\n",
        "    batch_size = 16\n",
        "    accumulation_steps = 2\n",
        "    learning_rate = 1.5e-5  # –£–≤–µ–ª–∏—á–∏–ª learning rate\n",
        "    epochs = 5  # –£–≤–µ–ª–∏—á–∏–ª –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\n",
        "    max_length = 300  # –£–≤–µ–ª–∏—á–∏–ª –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞\n",
        "    patience = 2  # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    num_warmup_ratio = 0.11  # –ü—Ä–æ–≥—Ä–µ–≤ –¥–ª—è —à–µ–¥—É–ª–µ—Ä–∞\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# ==================== –ö–ê–°–¢–û–ú–ù–ê–Ø –ú–û–î–ï–õ–¨ ====================\n",
        "class MultiLabelTransformerWithHeads(nn.Module):\n",
        "    \"\"\"–ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≥–æ–ª–æ–≤–∫–∞–º–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\"\"\"\n",
        "    def __init__(self, model_name, num_labels, hidden_dropout_prob=0.3):\n",
        "        super().__init__()\n",
        "        self.config = AutoConfig.from_pretrained(model_name)\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # –†–∞–∑–Ω—ã–µ –≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
        "        self.category_head = nn.Linear(self.config.hidden_size, num_labels)\n",
        "        self.genre_head = nn.Linear(self.config.hidden_size, num_labels)\n",
        "\n",
        "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
        "        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
        "\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–Ω—ã–µ –≥–æ–ª–æ–≤–∫–∏ –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º\n",
        "        normalized_output = self.layer_norm(pooled_output)\n",
        "        dropout_output = self.dropout(normalized_output)\n",
        "\n",
        "        logits1 = self.category_head(dropout_output)\n",
        "        logits2 = self.genre_head(dropout_output)\n",
        "\n",
        "        # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
        "        logits = (logits1 + logits2) / 2\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.BCEWithLogitsLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return type('Output', (), {'loss': loss, 'logits': logits})\n",
        "\n",
        "# ==================== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ====================\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —É–ª—É—á—à–µ–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
        "    train_data = pd.read_csv('/content/train.tsv', sep='\\t')\n",
        "    test_data = pd.read_csv('/content/test.tsv', sep='\\t')\n",
        "\n",
        "    print(\"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ train:\", train_data.columns.tolist())\n",
        "\n",
        "    # –£–õ–£–ß–®–ï–ù–ò–ï: –ë–æ–ª–µ–µ —É–º–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏\n",
        "    def create_enhanced_text(row):\n",
        "        parts = []\n",
        "\n",
        "        # 1. –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (—Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ)\n",
        "        if 'app_name' in row:\n",
        "            app_name = str(row['app_name']).strip()\n",
        "            if app_name and app_name != 'nan':\n",
        "                parts.append(f\"–ù–∞–∑–≤–∞–Ω–∏–µ: {app_name}\")\n",
        "\n",
        "        # 2. –ö–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\n",
        "        for col in ['shortDescription', 'short_description']:\n",
        "            if col in row and pd.notna(row[col]):\n",
        "                desc = str(row[col]).strip()\n",
        "                if desc and desc != 'nan':\n",
        "                    parts.append(f\"–û–ø–∏—Å–∞–Ω–∏–µ: {desc}\")\n",
        "                    break\n",
        "\n",
        "        # 3. –ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\n",
        "        for col in ['full_description', 'description', 'long_description']:\n",
        "            if col in row and pd.notna(row[col]):\n",
        "                full_desc = str(row[col]).strip()\n",
        "                if full_desc and full_desc != 'nan':\n",
        "                    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ–ª–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è\n",
        "                    parts.append(f\"–ü–æ–¥—Ä–æ–±–Ω–æ: {full_desc[:200]}\")\n",
        "                    break\n",
        "\n",
        "        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ\n",
        "        if not parts and 'app_name' in row:\n",
        "            return str(row['app_name'])\n",
        "\n",
        "        return \" \".join(parts)\n",
        "\n",
        "    train_data['text'] = train_data.apply(create_enhanced_text, axis=1)\n",
        "    test_data['text'] = test_data.apply(create_enhanced_text, axis=1)\n",
        "\n",
        "    # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤\n",
        "    train_data['text_length'] = train_data['text'].str.len()\n",
        "    print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {train_data['text_length'].mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
        "    print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {train_data['text_length'].max()} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
        "\n",
        "    # MultiLabelBinarizer –¥–ª—è –º–µ—Ç–æ–∫\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    train_labels = mlb.fit_transform(train_data['labels_str'].str.split('|'))\n",
        "\n",
        "    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ—Ç–æ–∫\n",
        "    label_counts = train_labels.sum(axis=0)\n",
        "    print(f\"\\nüìä –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–∫:\")\n",
        "    print(f\"–í—Å–µ–≥–æ –∫–ª–∞—Å—Å–æ–≤: {len(mlb.classes_)}\")\n",
        "    print(f\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫: {train_labels.sum()}\")\n",
        "    print(f\"–ú–µ—Ç–æ–∫ –Ω–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: {train_labels.sum(axis=1).mean():.2f}\")\n",
        "\n",
        "    # –í—ã–≤–æ–¥–∏–º —Ç–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –º–µ—Ç–æ–∫\n",
        "    top_labels = pd.DataFrame({\n",
        "        'category': mlb.classes_,\n",
        "        'count': label_counts\n",
        "    }).sort_values('count', ascending=False).head(10)\n",
        "    print(\"\\n–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:\")\n",
        "    print(top_labels)\n",
        "\n",
        "    return train_data, test_data, mlb, train_labels\n",
        "\n",
        "# ==================== –£–õ–£–ß–®–ï–ù–ù–´–ô –î–ê–¢–ê–°–ï–¢ –° –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ï–ô ====================\n",
        "class AdvancedTextAugmenter:\n",
        "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤\"\"\"\n",
        "    def __init__(self):\n",
        "        self.synonyms = {\n",
        "    '–∏–≥—Ä–∞': ['–≥–µ–π–º', '–∏–≥—Ä–æ–≤–æ–π', '–∏–≥—Ä–æ–≤–æ–µ', '—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏–µ', '–≤–∏–¥–µ–æ–∏–≥—Ä–∞', '–∞—Ä–∫–∞–¥–∞', '–≥–µ–π–º–∏–Ω–≥', '–∏–≥—Ä—É—à–∫–∞', '–±–∞—Ç–∞–ª–∏—è', '–ø–∞—Ä—Ç–∏—è', '—Å–æ—Å—Ç—è–∑–∞–Ω–∏–µ', '–∑–∞–±–∞–≤–∞', '–∞—Ç—Ç—Ä–∞–∫—Ü–∏–æ–Ω', '—Å–∏–º—É–ª—è—Ç–æ—Ä', '–∫–≤–µ—Å—Ç', '–≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', '—ç–∫—à–µ–Ω', '–ø—Ä–∏–∫–ª—é—á–µ–Ω–∏–µ'],\n",
        "    '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ': ['–ø—Ä–æ–≥—Ä–∞–º–º–∞', '—Å–æ—Ñ—Ç', '—É—Ç–∏–ª–∏—Ç–∞', '–∞–ø–ø', '–ø—Ä–∏–ª–æ–∂–µ–Ω—å–µ', '–ø—Ä–æ–≥—Ä–∞–º–º–∫–∞', '—Å–æ—Ñ—Ç–∏–Ω–∞', '–ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ', '–ü–û', '–∞–ø–ø–ª–∏–∫–∞—Ü–∏—è', '–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç', '–ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç', '—Å–æ—Ñ—Ç–≤–µ—Ä', '–∫–ª–∏–µ–Ω—Ç', '–ø—Ä–æ–≥–∞'],\n",
        "    '–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π': ['—Ñ—Ä–∏', '–¥–∞—Ä–æ–º', '–±–µ–∑ –æ–ø–ª–∞—Ç—ã', '–±–µ—Å–ø–ª–∞—Ç–Ω–æ', 'free', 'gratis', '–∑–∞ —Ç–∞–∫', '–±–µ—Å–ø–ª–∞—Ç–Ω–æ–µ', '–±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω—ã–π', '–¥–∞—Ä–æ–≤–æ–π', '–∫–æ–º–ø–ª–∏–º–µ–Ω—Ç–∞—Ä–Ω—ã–π', '—Ö–∞–ª—è–≤–Ω—ã–π', '–±–æ–Ω—É—Å–Ω—ã–π', '–ø–æ–¥–∞—Ä–æ—á–Ω—ã–π', '–Ω–µ–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π', '–æ—Ç–∫—Ä—ã—Ç—ã–π', '—Å–≤–æ–±–æ–¥–Ω—ã–π'],\n",
        "    '–æ–Ω–ª–∞–π–Ω': ['–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '—Å–µ—Ç–µ–≤–æ–π', '–≤–µ–±', 'online', '–≤ —Å–µ—Ç–∏', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç–Ω—ã–π', '—Å–µ—Ç–µ–≤–æ–µ', '–¥–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω—ã–π', '—É–¥–∞–ª–µ–Ω–Ω—ã–π', '–≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π', '–∫–∏–±–µ—Ä–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-', '–≤–µ–±-', '—Å–µ—Ç–µ–≤–æÃÅ–π', '–ø–æ–¥–∫–ª—é—á–µ–Ω–Ω—ã–π'],\n",
        "    '–æ–±—É—á–µ–Ω–∏–µ': ['–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '—É—á–µ–±–∞', '–∫—É—Ä—Å', '—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞', '–æ–±—É—á–∞—é—â–∏–π', '—É—á–µ–±–Ω—ã–π', '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '–ø–µ–¥–∞–≥–æ–≥–∏–∫–∞', '–Ω–∞—É–∫–∞', '–∏–Ω—Å—Ç—Ä—É–∫—Ç–∞–∂', '–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '–ø—Ä–æ—Å–≤–µ—â–µ–Ω–∏–µ', '–æ–±—É—á–∞–ª–∫–∞', '—Ç—Ä–µ–Ω–∏–Ω–≥', '—Å–µ–º–∏–Ω–∞—Ä', '–ª–µ–∫—Ü–∏–∏'],\n",
        "    '–º—É–∑—ã–∫–∞': ['–∞—É–¥–∏–æ', '–º–µ–ª–æ–¥–∏—è', '—Ç—Ä–µ–∫', '–ø–µ—Å–Ω—è', '–º—É–∑–ª–æ', '–∑–≤—É–∫', '–∫–æ–º–ø–æ–∑–∏—Ü–∏—è', '–º—É–∑—ã–∫–∞–ª—å–Ω—ã–π', '–º—É–∑—ã—á–∫–∞', '–Ω–∞–ø–µ–≤', '–º–æ—Ç–∏–≤', '–∞—Ä–∞–Ω–∂–∏—Ä–æ–≤–∫–∞', '—Å–∞—É–Ω–¥—Ç—Ä–µ–∫', '–º–∏–Ω—É—Å–æ–≤–∫–∞', '–±–∏—Ç', '—Ä–∏—Ç–º', '–≥–∞—Ä–º–æ–Ω–∏—è'],\n",
        "    '–≤–∏–¥–µ–æ': ['–∫–ª–∏–ø', '—Ä–æ–ª–∏–∫', '—Ñ–∏–ª—å–º', '–∫–∏–Ω–æ', '–≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫', '–≤–∏–¥–µ–æ–∫–ª–∏–ø', '–∫–∏–Ω–æ—Ñ–∏–ª—å–º', '–≤–∏–¥–µ–æ–∑–∞–ø–∏—Å—å', '–≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç', '–≤–∏–¥–µ–æ—Ñ–∞–π–ª', '–º—É–≤–∏', '—Ñ–∏–ª—å–º–µ—Ü', '—Ä–æ–ª–∏—á–µ–∫', '–∑–∞–ø–∏—Å—å', '—Ç—Ä–∞–Ω—Å–ª—è—Ü–∏—è', '—Å—Ç—Ä–∏–º'],\n",
        "    '—Ñ–æ—Ç–æ': ['–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', '–∫–∞—Ä—Ç–∏–Ω–∫–∞', '—Å–Ω–∏–º–æ–∫', '—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è', '—Ñ–æ—Ç–æ—Å–Ω–∏–º–æ–∫', '—Ñ–æ—Ç–∫–∞', '–∫–∞—Ä—Ç–æ—á–∫–∞', '—Ñ–æ—Ç–æ–∫–∞—Ä—Ç–æ—á–∫–∞', '—Å–Ω–∏–º–æ–∫', '–ø–∏–∫—á–∞', '—Ñ–æ—Ç–æ–¥–æ–∫—É–º–µ–Ω—Ç', '–∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è', '–ø–æ—Ä—Ç—Ä–µ—Ç', '–ø–µ–π–∑–∞–∂', '—Ñ–æ—Ç–æ–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'],\n",
        "    '—Å–æ—Ü–∏–∞–ª—å–Ω—ã–π': ['—Å–æ—Ü—Å–µ—Ç—å', '–æ–±—â–µ–Ω–∏–µ', '–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è', '—Å–æ—Ü–∏—É–º', '—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è —Å–µ—Ç—å', '–æ–±—â–∏–Ω–∞', '—Å–æ—Ü —Å–µ—Ç—å', '—Å–æ–æ–±—â–µ—Å—Ç–≤–æ', '—Å–µ—Ç—å', '–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞', '–º–µ–¥–∏–∞', '—Ñ–æ—Ä—É–º', '—á–∞—Ç', '–±–ª–æ–≥', '—Å–æ—Ü–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–æ–±—â–µ—Å—Ç–≤–æ'],\n",
        "    '–Ω–æ–≤–æ—Å—Ç–∏': ['—Å–æ–±—ã—Ç–∏—è', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è', '–Ω–æ–≤–æ—Å—Ç–Ω–æ–π', '–Ω–æ–≤–æ—Å—Ç–Ω–∞—è –ª–µ–Ω—Ç–∞', '—Å–≤–æ–¥–∫–∞', '–Ω–æ–≤–æ—Å—Ç–Ω–∏–∫', '—Ö—Ä–æ–Ω–∏–∫–∞', '—Ä–µ–ø–æ—Ä—Ç–∞–∂', '–∞–Ω–æ–Ω—Å', '–ø—Ä–µ—Å—Å–∞', '–º–µ–¥–∏–∞', '–ª–µ–Ω—Ç–∞', '–¥–∞–π–¥–∂–µ—Å—Ç', '–æ–±–∑–æ—Ä', '—Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏'],\n",
        "    '–º–∞–≥–∞–∑–∏–Ω': ['—à–æ–ø–∏–Ω–≥', '–ø–æ–∫—É–ø–∫–∏', '—Ç–æ—Ä–≥–æ–≤–ª—è', '–º–∞—Ä–∫–µ—Ç', '–æ–Ω–ª–∞–π–Ω –º–∞–≥–∞–∑–∏–Ω', '–º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å', '—Ç–æ—Ä–≥–æ–≤–∞—è –ø–ª–æ—â–∞–¥–∫–∞', '–±—É—Ç–∏–∫', '–ª–∞–≤–∫–∞', '—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω', '—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è', 'e-commerce', '—Ä–∏—Ç–µ–π–ª', '–ø—Ä–æ–¥–∞–∂–∏'],\n",
        "    '–∫–Ω–∏–≥–∞': ['–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '—á—Ç–µ–Ω–∏–µ', '–∏–∑–¥–∞–Ω–∏–µ', '–∫–Ω–∏–∂–Ω—ã–π', '—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–Ω–∏–≥–∞', '–±—É–∫', '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–µ', '—Ç–æ–º', '–∏–∑–¥–∞–Ω–∏–µ', '–ø—É–±–ª–∏–∫–∞—Ü–∏—è', '—Ä—É–∫–æ–ø–∏—Å—å', '—Ñ–æ–ª–∏–∞–Ω—Ç', '–±–µ—Å—Ç—Å–µ–ª–ª–µ—Ä', '—Ä–æ–º–∞–Ω', '–ø–æ–≤–µ—Å—Ç—å', '—Ä–∞—Å—Å–∫–∞–∑'],\n",
        "    '–∑–¥–æ—Ä–æ–≤—å–µ': ['–º–µ–¥–∏—Ü–∏–Ω–∞', '—Ñ–∏—Ç–Ω–µ—Å', '–∑–¥–æ—Ä–æ–≤—ã–π', '–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π', '–∑–¥–æ—Ä–æ–≤—å–µ –∏ —Ñ–∏—Ç–Ω–µ—Å', '–º–µ–¥', '–æ–∑–¥–æ—Ä–æ–≤–ª–µ–Ω–∏–µ', '–∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ', '–±–ª–∞–≥–æ–ø–æ–ª—É—á–∏–µ', '—Å–∞–º–æ—á—É–≤—Å—Ç–≤–∏–µ', '–≤–∏—Ç–∞–ª—å–Ω–æ—Å—Ç—å', '–≥–∏–≥–∏–µ–Ω–∞', '–ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞', '—Ä–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è'],\n",
        "    '–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ': ['—Ç—É—Ä–∏–∑–º', '–ø–æ–µ–∑–¥–∫–∞', '–æ—Ç–¥—ã—Ö', '—Ç—É—Ä', '–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è', '—Ç—Ä–∏–ø', '–≤–æ—è–∂', '—Ç—É—Ä–ø–æ–µ–∑–¥–∫–∞', '—ç–∫—Å–∫—É—Ä—Å–∏—è', '–ø–∞–ª–æ–º–Ω–∏—á–µ—Å—Ç–≤–æ', '–∫—Ä—É–∏–∑', '—Å–∞—Ñ–∞—Ä–∏', '—ç–∫—Å–ø–µ–¥–∏—Ü–∏—è', '–ø–æ—Ö–æ–¥', '–æ—Ç–ø—É—Å–∫', '–∫–∞–Ω–∏–∫—É–ª—ã'],\n",
        "    '–µ–¥–∞': ['–ø–∏—Ç–∞–Ω–∏–µ', '—Ä–µ—Ü–µ–ø—Ç', '–∫—É–ª–∏–Ω–∞—Ä–∏—è', '–ø–∏—â–∞', '–µ–¥–∞ –∏ –Ω–∞–ø–∏—Ç–∫–∏', '–∫—É—Ö–Ω—è', '–≥–æ—Ç–æ–≤–∫–∞', '–ø—Ä–æ–¥—É–∫—Ç—ã', '–±–ª—é–¥–æ', '–∫—É—à–∞–Ω—å–µ', '–≥–∞—Å—Ç—Ä–æ–Ω–æ–º–∏—è', '–¥–∏–µ—Ç–∞', '–º–µ–Ω—é', '—Ä–∞—Ü–∏–æ–Ω', '–ø—Ä–æ–≤–∏–∑–∏—è', '–∑–∞–∫—É—Å–∫–∞']\n",
        "}\n",
        "    def augment_text(self, text):\n",
        "        \"\"\"–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏\"\"\"\n",
        "        if random.random() < 0.6:  # 70% chance –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
        "            words = text.split()\n",
        "            if len(words) <= 3:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç - –Ω–µ –∞—É–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º\n",
        "                return text\n",
        "\n",
        "            augmented_words = []\n",
        "\n",
        "            for word in words:\n",
        "                word_lower = word.lower().strip('.,!?;:')\n",
        "\n",
        "                # –ó–∞–º–µ–Ω–∞ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 15%\n",
        "                if word_lower in self.synonyms and random.random() < 0.2:\n",
        "                    synonym = random.choice(self.synonyms[word_lower])\n",
        "                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–≥–∏—Å—Ç—Ä –ø–µ—Ä–≤–æ–π –±—É–∫–≤—ã\n",
        "                    if word[0].isupper():\n",
        "                        synonym = synonym.capitalize()\n",
        "                    augmented_words.append(synonym)\n",
        "                else:\n",
        "                    augmented_words.append(word)\n",
        "\n",
        "            return ' '.join(augmented_words)\n",
        "        return text\n",
        "\n",
        "class MultiStrategyAppDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=300, is_training=True,\n",
        "                 augment_prob=0.5, augment_strategies=['synonym', 'delete', 'swap']):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.is_training = is_training\n",
        "        self.augment_prob = augment_prob\n",
        "        self.augment_strategies = augment_strategies if is_training else []\n",
        "        self.augmenter = AdvancedTextAugmenter() if is_training else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def _apply_augmentation(self, text):\n",
        "        \"\"\"–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\"\"\"\n",
        "        if not self.augment_strategies or random.random() > self.augment_prob:\n",
        "            return text\n",
        "\n",
        "        strategy = random.choice(self.augment_strategies)\n",
        "\n",
        "        if strategy == 'synonym' and self.augmenter:\n",
        "            return self.augmenter.augment_text(text)\n",
        "\n",
        "        elif strategy == 'delete':\n",
        "            # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö —Å–ª–æ–≤ (10-20%)\n",
        "            words = text.split()\n",
        "            if len(words) > 5:\n",
        "                n_to_delete = max(1, int(len(words) * random.uniform(0.1, 0.2)))\n",
        "                indices_to_keep = random.sample(range(len(words)), len(words) - n_to_delete)\n",
        "                words = [words[i] for i in sorted(indices_to_keep)]\n",
        "                return ' '.join(words)\n",
        "            return text\n",
        "\n",
        "        elif strategy == 'swap':\n",
        "            # –°–≤–∞–ø —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–ª–æ–≤\n",
        "            words = text.split()\n",
        "            if len(words) > 3:\n",
        "                for _ in range(max(1, len(words) // 10)):\n",
        "                    i = random.randint(0, len(words) - 2)\n",
        "                    words[i], words[i + 1] = words[i + 1], words[i]\n",
        "                return ' '.join(words)\n",
        "            return text\n",
        "\n",
        "        elif strategy == 'repeat':\n",
        "            # –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç–µ–π\n",
        "            words = text.split()\n",
        "            if len(words) > 4:\n",
        "                # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 1-2 —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω—Ü–µ\n",
        "                n_repeat = random.randint(1, 2)\n",
        "                repeated = words[:n_repeat]\n",
        "                words.extend(repeated)\n",
        "                return ' '.join(words)\n",
        "            return text\n",
        "\n",
        "        return text\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é\n",
        "        if self.is_training:\n",
        "            text = self._apply_augmentation(text)\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "\n",
        "        return item\n",
        "\n",
        "# ==================== –£–õ–£–ß–®–ï–ù–ù–´–ï –§–£–ù–ö–¶–ò–ò –û–¶–ï–ù–ö–ò ====================\n",
        "def calculate_hit_at_k(predictions, true_labels, k=3):\n",
        "    \"\"\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ HitRate@K —Å —Ä–∞–∑–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏\"\"\"\n",
        "    hit_count = 0\n",
        "    for i in range(len(predictions)):\n",
        "        true_indices = set(np.where(true_labels[i] > 0)[0])\n",
        "\n",
        "        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –¢–æ–ø-K –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
        "        pred_indices = set(np.argsort(predictions[i])[-k:])\n",
        "\n",
        "        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ—Ä–æ–≥ + —Ç–æ–ø-K (—Ä–µ–∑–µ—Ä–≤–Ω–∞—è)\n",
        "        if not true_indices & pred_indices:\n",
        "            threshold = np.sort(predictions[i])[-k]  # k-–π –Ω–∞–∏–±–æ–ª—å—à–∏–π —Å–∫–æ—Ä\n",
        "            pred_indices_thresh = set(np.where(predictions[i] >= threshold)[0])\n",
        "            if true_indices & pred_indices_thresh:\n",
        "                hit_count += 1\n",
        "        else:\n",
        "            hit_count += 1\n",
        "\n",
        "    return hit_count / len(predictions)\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device) if 'labels' in batch else None\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.sigmoid(outputs.logits)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            if labels is not None:\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    if all_labels:\n",
        "        all_labels = np.vstack(all_labels)\n",
        "        hitrate_3 = calculate_hit_at_k(all_preds, all_labels, k=3)\n",
        "        hitrate_1 = calculate_hit_at_k(all_preds, all_labels, k=1)\n",
        "        hitrate_5 = calculate_hit_at_k(all_preds, all_labels, k=5)\n",
        "        return hitrate_3, hitrate_1, hitrate_5, all_preds, all_labels\n",
        "    else:\n",
        "        return None, None, None, all_preds, None\n",
        "\n",
        "# ==================== –£–õ–£–ß–®–ï–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï ====================\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss –¥–ª—è –±–æ—Ä—å–±—ã —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é –∫–ª–∞—Å—Å–æ–≤\"\"\"\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return F_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return F_loss.sum()\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, scheduler, device, accumulation_steps, criterion):\n",
        "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –ª–æ—Å—Å –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω, –∏–Ω–∞—á–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π\n",
        "        if criterion:\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "        else:\n",
        "            loss = outputs.loss\n",
        "\n",
        "        loss = loss / accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "        preds = torch.sigmoid(outputs.logits)\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            print(f\"  Batch {step}/{len(train_loader)}, Loss: {loss.item():.4f}, LR: {current_lr:.2e}\")\n",
        "\n",
        "    # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_labels = np.vstack(all_labels)\n",
        "    train_hitrate_3 = calculate_hit_at_k(all_preds, all_labels, k=3)\n",
        "    train_hitrate_1 = calculate_hit_at_k(all_preds, all_labels, k=1)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    return avg_loss, train_hitrate_3, train_hitrate_1\n",
        "\n",
        "# ==================== –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ====================\n",
        "def main():\n",
        "    print(\"üöÄ –ó–ê–ü–£–°–ö –£–õ–£–ß–®–ï–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø...\")\n",
        "    print(f\"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {config.device}\")\n",
        "    print(f\"–ú–æ–¥–µ–ª—å: {config.model_name}\")\n",
        "    print(f\"Batch size: {config.batch_size}\")\n",
        "    print(f\"Learning rate: {config.learning_rate}\")\n",
        "    print(f\"Max length: {config.max_length}\")\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "    train_data, test_data, mlb, train_labels = load_and_prepare_data()\n",
        "\n",
        "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –ö–ê–°–¢–û–ú–ù–û–ô –º–æ–¥–µ–ª–∏\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "    model = MultiLabelTransformerWithHeads(config.model_name, len(mlb.classes_))\n",
        "    model.to(config.device)\n",
        "\n",
        "    # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
        "    train_texts, val_texts, train_y, val_y = train_test_split(\n",
        "        train_data['text'].tolist(),\n",
        "        train_labels,\n",
        "        test_size=0.1,  # –£–º–µ–Ω—å—à–∏–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è –±–æ–ª—å—à–µ–≥–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n",
        "        random_state=42,\n",
        "        stratify=train_labels.argmax(axis=1)\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    print(f\"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ: {len(train_texts)}\")\n",
        "    print(f\"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ: {len(val_texts)}\")\n",
        "\n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
        "    train_dataset = MultiStrategyAppDataset(\n",
        "        train_texts, train_y, tokenizer, config.max_length,\n",
        "        is_training=True, augment_prob=0.6\n",
        "    )\n",
        "    val_dataset = MultiStrategyAppDataset(\n",
        "        val_texts, val_y, tokenizer, config.max_length,\n",
        "        is_training=False\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size * 2, shuffle=False)\n",
        "\n",
        "    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å —Ä–∞–∑–Ω—ã–º–∏ learning rates –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–µ–≤\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters()\n",
        "                   if not any(nd in n for nd in no_decay) and 'classifier' not in n and 'head' not in n],\n",
        "         'weight_decay': 0.01, 'lr': config.learning_rate},\n",
        "        {'params': [p for n, p in model.named_parameters()\n",
        "                   if any(nd in n for nd in no_decay) and 'classifier' not in n and 'head' not in n],\n",
        "         'weight_decay': 0.0, 'lr': config.learning_rate},\n",
        "        {'params': [p for n, p in model.named_parameters() if 'category_head' in n or 'genre_head' in n],\n",
        "         'weight_decay': 0.01, 'lr': config.learning_rate * 2},  # –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π LR –¥–ª—è –≥–æ–ª–æ–≤–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters)\n",
        "    total_steps = len(train_loader) * config.epochs // config.accumulation_steps\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(total_steps * config.num_warmup_ratio),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Focal Loss –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    criterion = FocalLoss(alpha=1, gamma=2)\n",
        "\n",
        "    # –û–±—É—á–µ–Ω–∏–µ —Å —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π\n",
        "    best_hitrate = 0.0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        print(f\"\\nüéØ –≠–ü–û–•–ê {epoch + 1}/{config.epochs}\")\n",
        "\n",
        "        # –û–±—É—á–µ–Ω–∏–µ\n",
        "        train_loss, train_hitrate_3, train_hitrate_1 = train_epoch(\n",
        "            model, train_loader, optimizer, scheduler, config.device,\n",
        "            config.accumulation_steps, criterion\n",
        "        )\n",
        "\n",
        "        # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
        "        val_hitrate_3, val_hitrate_1, val_hitrate_5, _, _ = evaluate_model(model, val_loader, config.device)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"Train H@1: {train_hitrate_1:.4f}, H@3: {train_hitrate_3:.4f}\")\n",
        "        print(f\"Val H@1: {val_hitrate_1:.4f}, H@3: {val_hitrate_3:.4f}, H@5: {val_hitrate_5:.4f}\")\n",
        "\n",
        "        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
        "        if val_hitrate_3 > best_hitrate:\n",
        "            best_hitrate = val_hitrate_3\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            torch.save(optimizer.state_dict(), 'best_optimizer.pth')\n",
        "            patience_counter = 0\n",
        "            print(f\"üéâ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨! H@3: {best_hitrate:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"‚è≥ –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞: {patience_counter}/{config.patience}\")\n",
        "\n",
        "        if patience_counter >= config.patience:\n",
        "            print(\"üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ä–∞–±–æ—Ç–∞–ª–∞!\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\n‚úÖ –õ–£–ß–®–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢: H@3 = {best_hitrate:.4f}\")\n",
        "\n",
        "    # ==================== –£–õ–£–ß–®–ï–ù–ù–û–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï ====================\n",
        "    print(\"\\nüîÆ –ó–ê–ì–†–£–ó–ö–ê –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò –î–õ–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô...\")\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    test_dataset = MultiStrategyAppDataset(\n",
        "        test_data['text'].tolist(),\n",
        "        None,\n",
        "        tokenizer,\n",
        "        config.max_length,\n",
        "        is_training=False\n",
        "    )\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size * 2, shuffle=False)\n",
        "\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ–º (TTA-like)\n",
        "    model.eval()\n",
        "    all_test_logits = []\n",
        "\n",
        "    print(\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(config.device)\n",
        "            attention_mask = batch['attention_mask'].to(config.device)\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            all_test_logits.append(logits.logits.cpu().numpy())\n",
        "\n",
        "    test_logits = np.vstack(all_test_logits)\n",
        "\n",
        "    # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–¢–†–ê–¢–ï–ì–ò–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô\n",
        "    class_names = mlb.classes_\n",
        "    predictions = []\n",
        "    confidence_scores = []\n",
        "\n",
        "    for i, logits_row in enumerate(test_logits):\n",
        "        probs = 1 / (1 + np.exp(-logits_row))  # sigmoid –≤—Ä—É—á–Ω—É—é –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è\n",
        "\n",
        "        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –¢–æ–ø-3 –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
        "        top3_indices = np.argsort(probs)[-3:][::-1]\n",
        "\n",
        "        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
        "        top_prob = probs[top3_indices[0]]\n",
        "        if top_prob < 0.3:  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å\n",
        "            # –ò—â–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞\n",
        "            high_conf_indices = np.where(probs > 0.2)[0]\n",
        "            if len(high_conf_indices) > 0:\n",
        "                top3_indices = high_conf_indices[np.argsort(-probs[high_conf_indices])][:3]\n",
        "            # –ï—Å–ª–∏ –≤—Å–µ —Ä–∞–≤–Ω–æ –Ω–µ—Ç, –±–µ—Ä–µ–º —Ç–æ–ø-3 –ø–æ –ª–æ–≥–∏—Ç–∞–º\n",
        "            if len(top3_indices) == 0:\n",
        "                top3_indices = np.argsort(logits_row)[-3:][::-1]\n",
        "\n",
        "        predicted_categories = [class_names[idx] for idx in top3_indices]\n",
        "        predictions.append(\"|\".join(predicted_categories))\n",
        "        confidence_scores.append(np.mean(probs[top3_indices]))\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "    submission = pd.DataFrame({\n",
        "        'app_name': test_data['app_name'],\n",
        "        'labels_str': predictions\n",
        "    })\n",
        "\n",
        "    submission.to_csv('enhanced_submission.tsv', sep='\\t', index=False)\n",
        "    print(\"üìÑ –£–õ–£–ß–®–ï–ù–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–û–•–†–ê–ù–ï–ù–´ –í enhanced_submission.tsv\")\n",
        "\n",
        "    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
        "    print(\"\\nüìä –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:\")\n",
        "    pred_counts = Counter([cat for pred in predictions for cat in pred.split('|')])\n",
        "    top_predicted = pd.DataFrame({\n",
        "        'category': list(pred_counts.keys()),\n",
        "        'count': list(pred_counts.values())\n",
        "    }).sort_values('count', ascending=False)\n",
        "\n",
        "    print(\"–¢–æ–ø-15 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:\")\n",
        "    print(top_predicted.head(15))\n",
        "\n",
        "    # –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
        "    print(f\"\\nüéØ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –£–í–ï–†–ï–ù–ù–û–°–¢–ò:\")\n",
        "    print(f\"–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {np.mean(confidence_scores):.3f}\")\n",
        "    print(f\"–ú–µ–¥–∏–∞–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {np.median(confidence_scores):.3f}\")\n",
        "    print(f\"–î–æ–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é > 0.5: {np.mean(np.array(confidence_scores) > 0.5):.3f}\")\n",
        "\n",
        "    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    train_label_counts = train_labels.sum(axis=0)\n",
        "    top_train_labels = pd.DataFrame({\n",
        "        'category': mlb.classes_,\n",
        "        'count': train_label_counts\n",
        "    }).sort_values('count', ascending=False).head(10)\n",
        "\n",
        "    print(\"\\nüìà –°–†–ê–í–ù–ï–ù–ò–ï –° –¢–†–ï–ù–ò–†–û–í–û–ß–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò:\")\n",
        "    print(\"–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    print(top_train_labels)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "da1816e5-552d-4710-816a-ec57acba8154",
        "id": "NdBIsoCHRVyU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ –ó–ê–ü–£–°–ö –£–õ–£–ß–®–ï–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø...\n",
            "–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n",
            "–ú–æ–¥–µ–ª—å: sberbank-ai/ruBert-base\n",
            "Batch size: 16\n",
            "Learning rate: 1.5e-05\n",
            "Max length: 300\n",
            "–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ train: ['app_name', 'full_description', 'shortDescription', 'labels_str']\n",
            "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: 285 —Å–∏–º–≤–æ–ª–æ–≤\n",
            "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: 425 —Å–∏–º–≤–æ–ª–æ–≤\n",
            "\n",
            "üìä –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–∫:\n",
            "–í—Å–µ–≥–æ –∫–ª–∞—Å—Å–æ–≤: 45\n",
            "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫: 63781\n",
            "–ú–µ—Ç–æ–∫ –Ω–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: 1.19\n",
            "\n",
            "–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:\n",
            "         category  count\n",
            "40          tools   6241\n",
            "11         casual   5269\n",
            "29         puzzle   4332\n",
            "4          arcade   3917\n",
            "14  entertainment   3739\n",
            "34      simulator   3124\n",
            "8        business   3076\n",
            "13      education   3076\n",
            "17   foodAndDrink   3005\n",
            "0          action   2219\n",
            "\n",
            "üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\n",
            "–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ: 48144\n",
            "–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ: 5350\n",
            "\n",
            "üéØ –≠–ü–û–•–ê 1/5\n",
            "  Batch 0/3009, Loss: 0.1045, LR: 0.00e+00\n",
            "  Batch 50/3009, Loss: 0.0942, LR: 4.53e-07\n",
            "  Batch 100/3009, Loss: 0.0788, LR: 9.07e-07\n",
            "  Batch 150/3009, Loss: 0.0621, LR: 1.36e-06\n",
            "  Batch 200/3009, Loss: 0.0448, LR: 1.81e-06\n",
            "  Batch 250/3009, Loss: 0.0282, LR: 2.27e-06\n",
            "  Batch 300/3009, Loss: 0.0260, LR: 2.72e-06\n",
            "  Batch 350/3009, Loss: 0.0208, LR: 3.17e-06\n",
            "  Batch 400/3009, Loss: 0.0184, LR: 3.63e-06\n",
            "  Batch 450/3009, Loss: 0.0185, LR: 4.08e-06\n",
            "  Batch 500/3009, Loss: 0.0182, LR: 4.53e-06\n",
            "  Batch 550/3009, Loss: 0.0168, LR: 4.99e-06\n",
            "  Batch 600/3009, Loss: 0.0179, LR: 5.44e-06\n",
            "  Batch 650/3009, Loss: 0.0148, LR: 5.89e-06\n",
            "  Batch 700/3009, Loss: 0.0170, LR: 6.35e-06\n",
            "  Batch 750/3009, Loss: 0.0158, LR: 6.80e-06\n",
            "  Batch 800/3009, Loss: 0.0136, LR: 7.26e-06\n",
            "  Batch 850/3009, Loss: 0.0138, LR: 7.71e-06\n",
            "  Batch 900/3009, Loss: 0.0154, LR: 8.16e-06\n",
            "  Batch 950/3009, Loss: 0.0144, LR: 8.62e-06\n",
            "  Batch 1000/3009, Loss: 0.0126, LR: 9.07e-06\n",
            "  Batch 1050/3009, Loss: 0.0132, LR: 9.52e-06\n",
            "  Batch 1100/3009, Loss: 0.0168, LR: 9.98e-06\n",
            "  Batch 1150/3009, Loss: 0.0137, LR: 1.04e-05\n",
            "  Batch 1200/3009, Loss: 0.0117, LR: 1.09e-05\n",
            "  Batch 1250/3009, Loss: 0.0109, LR: 1.13e-05\n",
            "  Batch 1300/3009, Loss: 0.0123, LR: 1.18e-05\n",
            "  Batch 1350/3009, Loss: 0.0089, LR: 1.22e-05\n",
            "  Batch 1400/3009, Loss: 0.0103, LR: 1.27e-05\n",
            "  Batch 1450/3009, Loss: 0.0114, LR: 1.31e-05\n",
            "  Batch 1500/3009, Loss: 0.0107, LR: 1.36e-05\n",
            "  Batch 1550/3009, Loss: 0.0085, LR: 1.41e-05\n",
            "  Batch 1600/3009, Loss: 0.0099, LR: 1.45e-05\n",
            "  Batch 1650/3009, Loss: 0.0079, LR: 1.50e-05\n",
            "  Batch 1700/3009, Loss: 0.0101, LR: 1.49e-05\n",
            "  Batch 1750/3009, Loss: 0.0093, LR: 1.49e-05\n",
            "  Batch 1800/3009, Loss: 0.0076, LR: 1.48e-05\n",
            "  Batch 1850/3009, Loss: 0.0104, LR: 1.48e-05\n",
            "  Batch 1900/3009, Loss: 0.0074, LR: 1.47e-05\n",
            "  Batch 1950/3009, Loss: 0.0103, LR: 1.47e-05\n",
            "  Batch 2000/3009, Loss: 0.0087, LR: 1.46e-05\n",
            "  Batch 2050/3009, Loss: 0.0094, LR: 1.46e-05\n",
            "  Batch 2100/3009, Loss: 0.0079, LR: 1.45e-05\n",
            "  Batch 2150/3009, Loss: 0.0079, LR: 1.44e-05\n",
            "  Batch 2200/3009, Loss: 0.0097, LR: 1.44e-05\n",
            "  Batch 2250/3009, Loss: 0.0082, LR: 1.43e-05\n",
            "  Batch 2300/3009, Loss: 0.0082, LR: 1.43e-05\n",
            "  Batch 2350/3009, Loss: 0.0106, LR: 1.42e-05\n",
            "  Batch 2400/3009, Loss: 0.0096, LR: 1.42e-05\n",
            "  Batch 2450/3009, Loss: 0.0060, LR: 1.41e-05\n",
            "  Batch 2500/3009, Loss: 0.0100, LR: 1.41e-05\n",
            "  Batch 2550/3009, Loss: 0.0096, LR: 1.40e-05\n",
            "  Batch 2600/3009, Loss: 0.0082, LR: 1.39e-05\n",
            "  Batch 2650/3009, Loss: 0.0093, LR: 1.39e-05\n",
            "  Batch 2700/3009, Loss: 0.0063, LR: 1.38e-05\n",
            "  Batch 2750/3009, Loss: 0.0082, LR: 1.38e-05\n",
            "  Batch 2800/3009, Loss: 0.0053, LR: 1.37e-05\n",
            "  Batch 2850/3009, Loss: 0.0097, LR: 1.37e-05\n",
            "  Batch 2900/3009, Loss: 0.0094, LR: 1.36e-05\n",
            "  Batch 2950/3009, Loss: 0.0067, LR: 1.35e-05\n",
            "  Batch 3000/3009, Loss: 0.0058, LR: 1.35e-05\n",
            "Train Loss: 0.0324\n",
            "Train H@1: 0.4121, H@3: 0.6204\n",
            "Val H@1: 0.6817, H@3: 0.8892, H@5: 0.9452\n",
            "üéâ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨! H@3: 0.8892\n",
            "\n",
            "üéØ –≠–ü–û–•–ê 2/5\n",
            "  Batch 0/3009, Loss: 0.0073, LR: 1.35e-05\n",
            "  Batch 50/3009, Loss: 0.0062, LR: 1.34e-05\n",
            "  Batch 100/3009, Loss: 0.0055, LR: 1.34e-05\n",
            "  Batch 150/3009, Loss: 0.0084, LR: 1.33e-05\n",
            "  Batch 200/3009, Loss: 0.0103, LR: 1.33e-05\n",
            "  Batch 250/3009, Loss: 0.0066, LR: 1.32e-05\n",
            "  Batch 300/3009, Loss: 0.0057, LR: 1.31e-05\n",
            "  Batch 350/3009, Loss: 0.0069, LR: 1.31e-05\n",
            "  Batch 400/3009, Loss: 0.0070, LR: 1.30e-05\n",
            "  Batch 450/3009, Loss: 0.0091, LR: 1.30e-05\n",
            "  Batch 500/3009, Loss: 0.0072, LR: 1.29e-05\n",
            "  Batch 550/3009, Loss: 0.0069, LR: 1.29e-05\n",
            "  Batch 600/3009, Loss: 0.0073, LR: 1.28e-05\n",
            "  Batch 650/3009, Loss: 0.0073, LR: 1.28e-05\n",
            "  Batch 700/3009, Loss: 0.0088, LR: 1.27e-05\n",
            "  Batch 750/3009, Loss: 0.0058, LR: 1.26e-05\n",
            "  Batch 800/3009, Loss: 0.0066, LR: 1.26e-05\n",
            "  Batch 850/3009, Loss: 0.0072, LR: 1.25e-05\n",
            "  Batch 900/3009, Loss: 0.0064, LR: 1.25e-05\n",
            "  Batch 950/3009, Loss: 0.0090, LR: 1.24e-05\n",
            "  Batch 1000/3009, Loss: 0.0063, LR: 1.24e-05\n",
            "  Batch 1050/3009, Loss: 0.0087, LR: 1.23e-05\n",
            "  Batch 1100/3009, Loss: 0.0047, LR: 1.23e-05\n",
            "  Batch 1150/3009, Loss: 0.0052, LR: 1.22e-05\n",
            "  Batch 1200/3009, Loss: 0.0064, LR: 1.21e-05\n",
            "  Batch 1250/3009, Loss: 0.0102, LR: 1.21e-05\n",
            "  Batch 1300/3009, Loss: 0.0052, LR: 1.20e-05\n",
            "  Batch 1350/3009, Loss: 0.0068, LR: 1.20e-05\n",
            "  Batch 1400/3009, Loss: 0.0038, LR: 1.19e-05\n",
            "  Batch 1450/3009, Loss: 0.0097, LR: 1.19e-05\n",
            "  Batch 1500/3009, Loss: 0.0078, LR: 1.18e-05\n",
            "  Batch 1550/3009, Loss: 0.0053, LR: 1.17e-05\n",
            "  Batch 1600/3009, Loss: 0.0068, LR: 1.17e-05\n",
            "  Batch 1650/3009, Loss: 0.0066, LR: 1.16e-05\n",
            "  Batch 1700/3009, Loss: 0.0045, LR: 1.16e-05\n",
            "  Batch 1750/3009, Loss: 0.0051, LR: 1.15e-05\n",
            "  Batch 1800/3009, Loss: 0.0046, LR: 1.15e-05\n",
            "  Batch 1850/3009, Loss: 0.0082, LR: 1.14e-05\n",
            "  Batch 1900/3009, Loss: 0.0064, LR: 1.14e-05\n",
            "  Batch 1950/3009, Loss: 0.0054, LR: 1.13e-05\n",
            "  Batch 2000/3009, Loss: 0.0040, LR: 1.12e-05\n",
            "  Batch 2050/3009, Loss: 0.0080, LR: 1.12e-05\n",
            "  Batch 2100/3009, Loss: 0.0050, LR: 1.11e-05\n",
            "  Batch 2150/3009, Loss: 0.0090, LR: 1.11e-05\n",
            "  Batch 2200/3009, Loss: 0.0054, LR: 1.10e-05\n",
            "  Batch 2250/3009, Loss: 0.0054, LR: 1.10e-05\n",
            "  Batch 2300/3009, Loss: 0.0097, LR: 1.09e-05\n",
            "  Batch 2350/3009, Loss: 0.0076, LR: 1.09e-05\n",
            "  Batch 2400/3009, Loss: 0.0057, LR: 1.08e-05\n",
            "  Batch 2450/3009, Loss: 0.0055, LR: 1.07e-05\n",
            "  Batch 2500/3009, Loss: 0.0055, LR: 1.07e-05\n",
            "  Batch 2550/3009, Loss: 0.0077, LR: 1.06e-05\n",
            "  Batch 2600/3009, Loss: 0.0047, LR: 1.06e-05\n",
            "  Batch 2650/3009, Loss: 0.0060, LR: 1.05e-05\n",
            "  Batch 2700/3009, Loss: 0.0062, LR: 1.05e-05\n",
            "  Batch 2750/3009, Loss: 0.0050, LR: 1.04e-05\n",
            "  Batch 2800/3009, Loss: 0.0062, LR: 1.03e-05\n",
            "  Batch 2850/3009, Loss: 0.0064, LR: 1.03e-05\n",
            "  Batch 2900/3009, Loss: 0.0051, LR: 1.02e-05\n",
            "  Batch 2950/3009, Loss: 0.0095, LR: 1.02e-05\n",
            "  Batch 3000/3009, Loss: 0.0072, LR: 1.01e-05\n",
            "Train Loss: 0.0135\n",
            "Train H@1: 0.6930, H@3: 0.9000\n",
            "Val H@1: 0.7166, H@3: 0.9138, H@5: 0.9628\n",
            "üéâ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨! H@3: 0.9138\n",
            "\n",
            "üéØ –≠–ü–û–•–ê 3/5\n",
            "  Batch 0/3009, Loss: 0.0084, LR: 1.01e-05\n",
            "  Batch 50/3009, Loss: 0.0058, LR: 1.01e-05\n",
            "  Batch 100/3009, Loss: 0.0066, LR: 1.00e-05\n",
            "  Batch 150/3009, Loss: 0.0034, LR: 9.95e-06\n",
            "  Batch 200/3009, Loss: 0.0067, LR: 9.89e-06\n",
            "  Batch 250/3009, Loss: 0.0054, LR: 9.83e-06\n",
            "  Batch 300/3009, Loss: 0.0042, LR: 9.78e-06\n",
            "  Batch 350/3009, Loss: 0.0057, LR: 9.72e-06\n",
            "  Batch 400/3009, Loss: 0.0065, LR: 9.67e-06\n",
            "  Batch 450/3009, Loss: 0.0065, LR: 9.61e-06\n",
            "  Batch 500/3009, Loss: 0.0061, LR: 9.55e-06\n",
            "  Batch 550/3009, Loss: 0.0053, LR: 9.50e-06\n",
            "  Batch 600/3009, Loss: 0.0052, LR: 9.44e-06\n",
            "  Batch 650/3009, Loss: 0.0065, LR: 9.39e-06\n",
            "  Batch 700/3009, Loss: 0.0068, LR: 9.33e-06\n",
            "  Batch 750/3009, Loss: 0.0078, LR: 9.27e-06\n",
            "  Batch 800/3009, Loss: 0.0038, LR: 9.22e-06\n",
            "  Batch 850/3009, Loss: 0.0048, LR: 9.16e-06\n",
            "  Batch 900/3009, Loss: 0.0042, LR: 9.11e-06\n",
            "  Batch 950/3009, Loss: 0.0065, LR: 9.05e-06\n",
            "  Batch 1000/3009, Loss: 0.0054, LR: 8.99e-06\n",
            "  Batch 1050/3009, Loss: 0.0053, LR: 8.94e-06\n",
            "  Batch 1100/3009, Loss: 0.0071, LR: 8.88e-06\n",
            "  Batch 1150/3009, Loss: 0.0073, LR: 8.83e-06\n",
            "  Batch 1200/3009, Loss: 0.0055, LR: 8.77e-06\n",
            "  Batch 1250/3009, Loss: 0.0048, LR: 8.71e-06\n",
            "  Batch 1300/3009, Loss: 0.0054, LR: 8.66e-06\n",
            "  Batch 1350/3009, Loss: 0.0104, LR: 8.60e-06\n",
            "  Batch 1400/3009, Loss: 0.0095, LR: 8.55e-06\n",
            "  Batch 1450/3009, Loss: 0.0060, LR: 8.49e-06\n",
            "  Batch 1500/3009, Loss: 0.0060, LR: 8.43e-06\n",
            "  Batch 1550/3009, Loss: 0.0053, LR: 8.38e-06\n",
            "  Batch 1600/3009, Loss: 0.0080, LR: 8.32e-06\n",
            "  Batch 1650/3009, Loss: 0.0040, LR: 8.27e-06\n",
            "  Batch 1700/3009, Loss: 0.0043, LR: 8.21e-06\n",
            "  Batch 1750/3009, Loss: 0.0075, LR: 8.15e-06\n",
            "  Batch 1800/3009, Loss: 0.0040, LR: 8.10e-06\n",
            "  Batch 1850/3009, Loss: 0.0065, LR: 8.04e-06\n",
            "  Batch 1900/3009, Loss: 0.0044, LR: 7.99e-06\n",
            "  Batch 1950/3009, Loss: 0.0047, LR: 7.93e-06\n",
            "  Batch 2000/3009, Loss: 0.0045, LR: 7.87e-06\n",
            "  Batch 2050/3009, Loss: 0.0063, LR: 7.82e-06\n",
            "  Batch 2100/3009, Loss: 0.0052, LR: 7.76e-06\n",
            "  Batch 2150/3009, Loss: 0.0108, LR: 7.71e-06\n",
            "  Batch 2200/3009, Loss: 0.0051, LR: 7.65e-06\n",
            "  Batch 2250/3009, Loss: 0.0058, LR: 7.59e-06\n",
            "  Batch 2300/3009, Loss: 0.0041, LR: 7.54e-06\n",
            "  Batch 2350/3009, Loss: 0.0081, LR: 7.48e-06\n",
            "  Batch 2400/3009, Loss: 0.0073, LR: 7.42e-06\n",
            "  Batch 2450/3009, Loss: 0.0037, LR: 7.37e-06\n",
            "  Batch 2500/3009, Loss: 0.0045, LR: 7.31e-06\n",
            "  Batch 2550/3009, Loss: 0.0050, LR: 7.26e-06\n",
            "  Batch 2600/3009, Loss: 0.0077, LR: 7.20e-06\n",
            "  Batch 2650/3009, Loss: 0.0052, LR: 7.14e-06\n",
            "  Batch 2700/3009, Loss: 0.0054, LR: 7.09e-06\n",
            "  Batch 2750/3009, Loss: 0.0063, LR: 7.03e-06\n",
            "  Batch 2800/3009, Loss: 0.0030, LR: 6.98e-06\n",
            "  Batch 2850/3009, Loss: 0.0037, LR: 6.92e-06\n",
            "  Batch 2900/3009, Loss: 0.0039, LR: 6.86e-06\n",
            "  Batch 2950/3009, Loss: 0.0052, LR: 6.81e-06\n",
            "  Batch 3000/3009, Loss: 0.0056, LR: 6.75e-06\n",
            "Train Loss: 0.0116\n",
            "Train H@1: 0.7431, H@3: 0.9303\n",
            "Val H@1: 0.7284, H@3: 0.9228, H@5: 0.9619\n",
            "üéâ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨! H@3: 0.9228\n",
            "\n",
            "üéØ –≠–ü–û–•–ê 4/5\n",
            "  Batch 0/3009, Loss: 0.0054, LR: 6.74e-06\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1532478889.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1532478889.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# –û–±—É—á–µ–Ω–∏–µ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         train_loss, train_hitrate_3, train_hitrate_1 = train_epoch(\n\u001b[0m\u001b[1;32m    464\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulation_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1532478889.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, scheduler, device, accumulation_steps, criterion)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, BertModel, AutoConfig\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================\n",
        "class Config:\n",
        "    model_name = \"sberbank-ai/ruBert-base\"\n",
        "    batch_size = 16\n",
        "    max_length = 300\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# ==================== –ú–û–î–ï–õ–¨ ====================\n",
        "class MultiLabelTransformerWithHeads(nn.Module):\n",
        "    def __init__(self, model_name, num_labels, hidden_dropout_prob=0.3):\n",
        "        super().__init__()\n",
        "        self.config = AutoConfig.from_pretrained(model_name)\n",
        "        self.backbone = BertModel.from_pretrained(model_name)\n",
        "        self.category_head = nn.Linear(self.config.hidden_size, num_labels)\n",
        "        self.genre_head = nn.Linear(self.config.hidden_size, num_labels)\n",
        "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
        "        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0]\n",
        "        normalized_output = self.layer_norm(pooled_output)\n",
        "        dropout_output = self.dropout(normalized_output)\n",
        "        logits1 = self.category_head(dropout_output)\n",
        "        logits2 = self.genre_head(dropout_output)\n",
        "        logits = (logits1 + logits2) / 2\n",
        "        return logits\n",
        "\n",
        "# ==================== –î–ê–¢–ê–°–ï–¢ ====================\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=300):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n",
        "# ==================== –§–£–ù–ö–¶–ò–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø ====================\n",
        "def predict_on_test_data():\n",
        "    print(\"üöÄ –ó–ê–ü–£–°–ö –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•...\")\n",
        "    print(f\"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {config.device}\")\n",
        "    print(f\"–ú–æ–¥–µ–ª—å: {config.model_name}\")\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    try:\n",
        "        test_data = pd.read_csv('/content/test.tsv', sep='\\t')\n",
        "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {len(test_data)} —Å—Ç—Ä–æ–∫\")\n",
        "    except:\n",
        "        # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ\n",
        "        print(\"‚ö†Ô∏è –§–∞–π–ª test.tsv –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–æ–∑–¥–∞–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ...\")\n",
        "        test_data = pd.DataFrame({\n",
        "            'app_name': [f'App_{i}' for i in range(100)],\n",
        "            'description': [f'–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è {i}' for i in range(100)]\n",
        "        })\n",
        "\n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    def create_text(row):\n",
        "        parts = []\n",
        "        if 'app_name' in row:\n",
        "            app_name = str(row['app_name']).strip()\n",
        "            if app_name and app_name != 'nan':\n",
        "                parts.append(f\"–ù–∞–∑–≤–∞–Ω–∏–µ: {app_name}\")\n",
        "\n",
        "        for col in ['shortDescription', 'short_description', 'description', 'full_description']:\n",
        "            if col in row and pd.notna(row[col]):\n",
        "                desc = str(row[col]).strip()\n",
        "                if desc and desc != 'nan':\n",
        "                    parts.append(f\"–û–ø–∏—Å–∞–Ω–∏–µ: {desc}\")\n",
        "                    break\n",
        "\n",
        "        if not parts:\n",
        "            return \"–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\"\n",
        "        return \" \".join(parts)\n",
        "\n",
        "    test_data['text'] = test_data.apply(create_text, axis=1)\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "    test_dataset = TestDataset(test_data['text'].tolist(), tokenizer, config.max_length)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "\n",
        "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ (–Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Ä—É—á–Ω—É—é)\n",
        "    try:\n",
        "        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å mlb –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –∑–∞–Ω–æ–≤–æ\n",
        "        train_data = pd.read_csv('/content/train.tsv', sep='\\t')\n",
        "        mlb = MultiLabelBinarizer()\n",
        "        mlb.fit(train_data['labels_str'].str.split('|'))\n",
        "        num_classes = len(mlb.classes_)\n",
        "        print(f\"–ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Å–æ–≤: {num_classes}\")\n",
        "    except:\n",
        "        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã\n",
        "        default_categories = ['–∏–≥—Ä–∞', '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '–º—É–∑—ã–∫–∞', '–≤–∏–¥–µ–æ', '—Å–æ—Ü–∏–∞–ª—å–Ω—ã–π',\n",
        "                            '–Ω–æ–≤–æ—Å—Ç–∏', '–º–∞–≥–∞–∑–∏–Ω', '–∫–Ω–∏–≥–∞', '–∑–¥–æ—Ä–æ–≤—å–µ', '–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ']\n",
        "        mlb = MultiLabelBinarizer()\n",
        "        mlb.fit([default_categories])  # –§–∏—Ç–∏—Ä—É–µ–º –Ω–∞ –≤—Å–µ–º —Å–ø–∏—Å–∫–µ\n",
        "        num_classes = len(mlb.classes_)\n",
        "        print(f\"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã: {num_classes}\")\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "    model = MultiLabelTransformerWithHeads(config.model_name, num_classes)\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('/content/drive/MyDrive/best_model (1).pth', map_location=config.device))\n",
        "        print(\"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ best_model.pth\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå –§–∞–π–ª best_model.pth –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
        "        print(\"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}\")\n",
        "        return\n",
        "\n",
        "    model.to(config.device)\n",
        "    model.eval()\n",
        "\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    all_predictions = []\n",
        "\n",
        "    print(\"üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(config.device)\n",
        "            attention_mask = batch['attention_mask'].to(config.device)\n",
        "\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            probabilities = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "            # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
        "            for prob_row in probabilities:\n",
        "                top_3_indices = np.argsort(prob_row)[-3:][::-1]  # –ò–Ω–¥–µ–∫—Å—ã —Ç–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
        "                predicted_categories = [mlb.classes_[idx] for idx in top_3_indices]\n",
        "                all_predictions.append(\"|\".join(predicted_categories))\n",
        "\n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∞–±–º–∏—Ç–∞\n",
        "    submission = pd.DataFrame({\n",
        "        'app_name': test_data['app_name'],\n",
        "        'labels_str': all_predictions\n",
        "    })\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "    submission_file = 'test_predictions.tsv'\n",
        "    submission.to_csv(submission_file, sep='\\t', index=False)\n",
        "    print(f\"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {submission_file}\")\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
        "    print(\"\\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:\")\n",
        "    print(f\"–í—Å–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: {len(submission)} —Å—Ç—Ä–æ–∫\")\n",
        "\n",
        "    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
        "    all_predicted_categories = []\n",
        "    for pred in all_predictions:\n",
        "        all_predicted_categories.extend(pred.split('|'))\n",
        "\n",
        "    from collections import Counter\n",
        "    category_counts = Counter(all_predicted_categories)\n",
        "\n",
        "    print(\"\\n–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:\")\n",
        "    for category, count in category_counts.most_common(10):\n",
        "        print(f\"  {category}: {count} —Ä–∞–∑\")\n",
        "\n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
        "    print(f\"\\nüìù –ü–ï–†–í–´–ï 10 –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:\")\n",
        "    for i in range(min(10, len(submission))):\n",
        "        print(f\"  {i+1}. {submission.iloc[i]['app_name']} -> {submission.iloc[i]['labels_str']}\")\n",
        "\n",
        "    return submission\n",
        "\n",
        "# ==================== –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–ê–Ø –ü–†–û–°–¢–ê–Ø –ú–û–î–ï–õ–¨ ====================\n",
        "class SimpleBertClassifier(nn.Module):\n",
        "    \"\"\"–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç\"\"\"\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "def predict_with_simple_model():\n",
        "    \"\"\"–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é\"\"\"\n",
        "    print(\"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...\")\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏)\n",
        "    test_data = pd.read_csv('/content/test.tsv', sep='\\t')\n",
        "    test_data['text'] = test_data.apply(\n",
        "        lambda x: f\"{x['app_name']} {x.get('description', '')}\", axis=1\n",
        "    )\n",
        "\n",
        "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤\n",
        "    num_classes = 50  # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "    model = SimpleBertClassifier(config.model_name, num_classes)\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('best_model.pth', map_location=config.device))\n",
        "        print(\"‚úÖ –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ\")\n",
        "    except:\n",
        "        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É.\")\n",
        "        return\n",
        "\n",
        "    # –û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏...\n",
        "    return predict_on_test_data()\n",
        "\n",
        "# ==================== –ó–ê–ü–£–°–ö ====================\n",
        "if __name__ == \"__main__\":\n",
        "    # –ü—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é\n",
        "    try:\n",
        "        result = predict_on_test_data()\n",
        "        print(\"\\nüéâ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–´!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: {e}\")\n",
        "        print(\"–ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥...\")\n",
        "        try:\n",
        "            result = predict_with_simple_model()\n",
        "        except Exception as e2:\n",
        "            print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: {e2}\")\n",
        "            print(\"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ:\")\n",
        "            print(\"1. –ù–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ best_model.pth\")\n",
        "            print(\"2. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏\")\n",
        "            print(\"3. –ù–∞–ª–∏—á–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viJxd6xAvQHq",
        "outputId": "45706518-88c8-47ef-9bd8-0670da7d8320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ –ó–ê–ü–£–°–ö –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô –ù–ê –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•...\n",
            "–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n",
            "–ú–æ–¥–µ–ª—å: sberbank-ai/ruBert-base\n",
            "–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: 15046 —Å—Ç—Ä–æ–∫\n",
            "–ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Å–æ–≤: 45\n",
            "‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ best_model.pth\n",
            "üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\n",
            "‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ test_predictions.tsv\n",
            "\n",
            "üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:\n",
            "–í—Å–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ: 15046 —Å—Ç—Ä–æ–∫\n",
            "\n",
            "–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:\n",
            "  casual: 4232 —Ä–∞–∑\n",
            "  tools: 4192 —Ä–∞–∑\n",
            "  arcade: 3639 —Ä–∞–∑\n",
            "  entertainment: 3098 —Ä–∞–∑\n",
            "  business: 2924 —Ä–∞–∑\n",
            "  simulator: 2407 —Ä–∞–∑\n",
            "  puzzle: 2259 —Ä–∞–∑\n",
            "  education: 1972 —Ä–∞–∑\n",
            "  action: 1850 —Ä–∞–∑\n",
            "  lifestyle: 1691 —Ä–∞–∑\n",
            "\n",
            "üìù –ü–ï–†–í–´–ï 10 –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:\n",
            "  1. Lemon clicker -> casual|arcade|simulator\n",
            "  2. Memo –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫ -> education|books|tools\n",
            "  3. Slave Man Rescue -> puzzle|adventure|arcade\n",
            "  4. Taking Care of Granny -> casual|arcade|children\n",
            "  5. Escape From Classic Room -> puzzle|casual|arcade\n",
            "  6. Boy Rescue From Ghost -> adventure|action|puzzle\n",
            "  7. Charmer Escape -> puzzle|casual|arcade\n",
            "  8. Jolly Theatre Escape 2 -> puzzle|adventure|casual\n",
            "  9. Modern Purple House Escape -> puzzle|simulator|casual\n",
            "  10. Fun Escape Games 2002 -> puzzle|arcade|adventure\n",
            "\n",
            "üéâ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–´!\n"
          ]
        }
      ]
    }
  ]
}